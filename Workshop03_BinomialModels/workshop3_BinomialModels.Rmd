---
title: "Workshop 3 -- Binomial model"
author: "Frederic Barraquand"
date: "March 21, 2020"
output: 
  html_document:
      toc: true
      toc_depth: 4
      toc_float:
        collapsed: false
      theme: paper
      highlight: textmate
---

```{r setup, include=FALSE}
options(width = 300)
knitr::opts_chunk$set(cache = TRUE)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(MASS)
```

## Binomial and Bernoulli (generalized linear) models on simulated data 

Here we construct a Binomial model first. We have got 25 groups of size 50 or less, with a slightly different probability of success in each group. Let's say we have we have 25 groups of $z_i$ turtles. Females are born with probability $p_i$ in group $i$. 

```{r simulating-data}
sample_size_per_group = round(30*runif(25))+20
n_groups = length(sample_size_per_group)
temperature = (1:n_groups)*0.1 + rnorm(n_groups,0,0.5)
plot(1:n_groups,temperature,type="o",xlab="Group ID",ylab="Temperature")

Y = p = temperature
for (i in 1:n_groups){
  p[i] = 1/(1+exp(-3*(temperature[i]-mean(temperature)) ) )
  Y[i] = rbinom(1,sample_size_per_group[i],p[i])
}

plot(temperature,p,type="p",xlab="Temperature",ylab="Pr(female)")
plot(1:n_groups,Y,type="p",xlab="Number_group",ylab="N_females")

```

Data in a list:

```{r data_m1}
m11.data <- list(N = n_groups, y = Y, temp = temperature, z = sample_size_per_group)
```

Now we fit that model which writes mathematically like

$$ y_i \sim \text{Binomial}(z_i,p(\text{temp}_i)) $$

```{stan output.var="m1.1"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  vector[N] temp;                                  // temperature
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
}
parameters {                                       // unobserved variables
  real mu_temp;
  real gamma; 
}
model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(gamma*(temp[k]-mu_temp)));       // likelihood
  //y[k] ~ bernoulli_logit(alpha+beta*temp[k]);       // likelihood -- does not work because not bernoulli of course!
  }
  mu_temp ~ normal(2, 10);        // prior of the mean temp
  gamma ~ normal(1, 10);          // prior of the slope
}
```

```{r print_m1.1}
fit.m1.1 <- sampling(m1.1, data = m11.data, iter = 1000, chains = 2, cores = 2)
print(fit.m1.1, probs = c(0.10, 0.5, 0.9))
```

Now we consider a simpler model with a different set of priors. We consider only one group which has 50 young turtles. The probability of obtaining a female of 0.3. 
We have individual-level data. (As we don't have individual-level covariates, doing a bernoulli or binomial model is very much similar, but I am changing things just for fun and exercise here. Also to see if some codes are much faster). 

```{r simulating-data-2}
Y=rbinom(50,1,0.3)
m12.data <- list(N = 50, y = Y)

```

And now we fit the model

```{r stan-model-2}
m1.2 = stan_model("m1.2.stan")
fit.m1.2 <- sampling(m1.2, data = m12.data, iter = 1000, chains = 2, cores = 2)
print(fit.m1.2, probs = c(0.10, 0.5, 0.9))

```


## Playing with priors

### Why Beta? 

Why did I choose the Beta distribution for the prior of $p$? In short, this is the *conjugate prior distribution*, some sort of canonical choice. In the past, choosing conjugate priors used to speed up monumentously the computations. Nowadays this is less true but still: 
- A conjugate prior is a prior whose probability distribution is also the distribution of the posterior. 
In other words, here if we have a Beta prior we get a Beta posterior, and if we iterate the data assimilation process 10 000 times it will still be Beta. 

[For more info and formulas](https://en.wikipedia.org/wiki/Conjugate_prior)

### What shape? 

```{r shape-beta}
curve(dbeta(x,2,2))
curve(dbeta(x,0.5,0.5),add=T,col="red")
```

Obivously red would make little sense here. What would it imply? 

```{r modified-priors}
m1.3 = stan_model("m1.3.stan")
fit.m1.3 <- sampling(m1.3, data = m12.data, iter = 1000, chains = 2, cores = 2)
print(fit.m1.3, probs = c(0.10, 0.5, 0.9))
```

Still works well but the prior is bringing no information or even poor information. 

### The consequence of too flat priors (esp. when messing with link functions)

```{stan output.var="m1.5"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  vector[N] temp;                                  // temperature
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
}
parameters {                                       // unobserved variables
  real mu_temp;
  real gamma; 
}
model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(gamma*(temp[k]-mu_temp)));       // likelihood
  //y[k] ~ bernoulli_logit(alpha+beta*temp[k]);       // likelihood -- does not work because not bernoulli of course!
  }
  mu_temp ~ normal(2, 100);        // prior of the mean temp
  gamma ~ normal(1, 100);          // prior of the slope
}
```

Let us propagate the uncertainty by visualizing the transformed parameter. We simulate according to the prior

```{r simulating-prior-and-transformed-quantities}
mu_temp = rnorm(100,2, 100) # prior of the mean temp
gamma = rnorm(100,1, 100)   # prior on the slope
x=seq(min(temperature),max(temperature),by=0.01)
par(mfrow=c(1,2))
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-0.3*(x-mu_temp[kprior])) ) 
  lines(x,prob,type="l",col="blue")}
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-gamma[kprior]*(x-mean(temperature))) ) 
  lines(x,prob,type="l",col="blue")}
### We get either 0 or 1. 

### Better priors
mu_temp = rnorm(100,2, 1) # prior of the mean temp
gamma = rnorm(100,1, 1)   # prior on the slope
par(mfrow=c(1,2))
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-0.3*(x-mu_temp[kprior])) ) 
  lines(x,prob,type="l",col="blue")}
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-abs(gamma[kprior])*(x-mean(temperature))) ) 
  lines(x,prob,type="l",col="blue")}

```


### Weakly informative priors

[What are they? Why use them? A more detailed explanation](https://onlinelibrary.wiley.com/doi/10.1111/oik.05985)

## A real data example: eagles

As suggested by McElreath p. 330 of his book, records of (160!) salmon-pirating attempts by one Bald eagle on another Bald eagle (not always the same!). NB: I haven't implemented the quadratic approx. suggested by McElreath though that sounds like an excellent exercise. 

```{r eagles-exploring data}
?eagles
data(eagles) ### Explanatory variables
#P
#Size of pirating eagle (L = large, S = small).
#A
#Age of pirating eagle (I = immature, A = adult).
#V
#Size of victim eagle (L = large, S = small).
eagles.glm <- glm(cbind(y, n - y) ~ P*A + V, data = eagles,
                  family = binomial) ##classic frequentist modelling
# Small and immature birds capture less, small birds are more likely to have salmon stolen. So far so good. 
m2.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,P=as.numeric(eagles$P)-1,A=as.numeric(eagles$A)-1,V=as.numeric(eagles$V)-1)
# m2.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,P=eagles$P,A=eagles$A,V=eagles$V)

```

We're in a similar situation to the turtles sex, but with categorical explanatory variables

```{stan output.var="m2.1"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
  int<lower=0,upper=1>  A[N];                      // attribute age of pirating eagle
  int<lower=0,upper=1>  P[N];                      // attribute size of pirating eagle
  int<lower=0,upper=1>  V[N];                      // attribute size of victim eagle
  
}

parameters {                                       // unobserved variables
  real alpha;
  real beta_P; 
  real beta_V; 
  real beta_A; 
}

model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(alpha + beta_P*P[k] + beta_V*V[k] +beta_A*A[k]) );       // likelihood
  }
  alpha ~ normal(0, 10);        // prior of the mean temp
  beta_P ~ normal(0, 5);          // prior of the slope
  beta_V ~ normal(0, 5);          // prior of the slope
  beta_A ~ normal(0, 5);          // prior of the slope
}

```

Fitting the model

```{r print_m2.1}
m2.1 = stan_model("m2.1.stan",verbose = TRUE)
fit.m2.1 <- sampling(m2.1, data = m2.data, iter = 1000, chains = 2, cores = 2)
print(fit.m2.1, probs = c(0.10, 0.5, 0.9))
```

What do we think of these priors? 


## A model with hierarchical priors (hyperprior)

The stan book promotes this model

```{stan output.var="m3.1"}

data {
  int<lower=1> D;
  int<lower=0> N;
  int<lower=1> L;
  int<lower=0,upper=1> y[N];  // raw data
  int<lower=1,upper=L> ll[N]; // set of levels related to each data point
  row_vector[D] x[N];
}
parameters {
  real mu[D];                
  real<lower=0> sigma[D];
  vector[D] beta[L];
}
model {
  for (d in 1:D) {
    mu[d] ~ normal(0, 100);
    for (l in 1:L)
      beta[l,d] ~ normal(mu[d], sigma[d]);
  }
  for (n in 1:N)
    y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));
}

``` 

We've got the opportunity to transform the previously eagles model into something less complex but with a similar philosophy (NB I wondering what priors they have on sigma[d] in their example). Currently we have estimated each effect (size of pirating eagle, age of pirating eagle, size of victim eagle) independently. On relatively small datasets, this is prone to inflation of effects and why in practice many statisticians implement some of form of [shrinkage](https://en.wikipedia.org/wiki/Shrinkage_(statistics)) in such models. This is done easily with an additional prior linking together the effects (see also McElreath's book on this).

```{stan output.var="m3.2"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
  int<lower=0,upper=1>  A[N];
  int<lower=0,upper=1>  P[N];
  int<lower=0,upper=1>  V[N];
  
}

parameters {                                       // unobserved variables
  real alpha;
  real beta_P; 
  real beta_V; 
  real beta_A; 
  real gamma_b; 
}

model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(alpha + beta_P*P[k] + beta_V*V[k] +beta_A*A[k]) );       // likelihood
  }
  alpha ~ normal(0, 1);                // prior of the mean 
  beta_P ~ normal(0, gamma_b);          // prior of the "slope" (here the effect, rather)
  beta_V ~ normal(0, gamma_b);          // prior of the slope
  beta_A ~ normal(0, gamma_b);          // prior of the slope
  gamma_b ~ gamma(0.1,0.1); 		// hyperprior
}

``` 

Let us now fit that model

```{r fitting-hyperprior-model}
m3.2 = stan_model("m3.2.stan")
fit.m3.2 <- sampling(m3.2, data = m2.data, iter = 1000, chains = 2, cores = 2)
print(fit.m3.2, probs = c(0.10, 0.5, 0.9))
```

Very moderate amount of shrinkage. There are some divergent iterations as well. 
Perhaps there are not enough effects for this, or the distribution - gamma - is not constraining enough. 



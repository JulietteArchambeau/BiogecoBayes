---
title: "Workshop 3 -- Binomial model"
author: "Frederic Barraquand"
date: "March 21, 2020"
output: html_document
---

```{r setup, include=FALSE}
options(width = 300)
library(rstan)
library(MASS)
```

## Binomial and Bernoulli (generalized linear) models on simulated data 

Here we construct a Binomial model first. We have got 25 groups of size 50 or less, with a slightly different probability of success in each group. Let's say we have we have 25 groups of $z_i$ turtles. Females are born with probability $p_i$ in group $i$. 

```{r simulating-data}
sample_size_per_group = round(30*runif(25))+20
n_groups = length(sample_size_per_group)
temperature = (1:n_groups)*0.1 + rnorm(n_groups,0,0.5)
plot(1:n_groups,temperature,type="o",xlab="Group ID",ylab="Temperature")

Y = p = temperature
for (i in 1:n_groups){
  p[i] = 1/(1+exp(-3*(temperature[i]-mean(temperature)) ) )
  Y[i] = rbinom(1,sample_size_per_group[i],p[i])
}

plot(temperature,p,type="p",xlab="Temperature",ylab="Pr(female)")
plot(1:n_groups,Y,type="p",xlab="Number_group",ylab="N_females")

```

Data in a list:

```{r data_m1}
m1.data <- list(N = n_groups, y = Y, temp = temperature, z = sample_size_per_group)
```

Now we fit that model which writes mathematically like

$$ y_i \sim \text{Binomial}(z_i,p(\text{temp}_i)) $$

```{stan output.var="m1.1"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  vector[N] temp;                                  // temperature
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
}
parameters {                                       // unobserved variables
  real mu_temp;
  real gamma; 
}
model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(gamma*(temp[k]-mu_temp)));       // likelihood
  //y[k] ~ bernoulli_logit(alpha+beta*temp[k]);       // likelihood -- does not work because not bernoulli of course!
  }
  mu_temp ~ normal(2, 10);        // prior of the mean temp
  gamma ~ normal(1, 10);          // prior of the slope
}
```

```{r print_m1.1}
fit.m1.1 <- sampling(m1.1, data = m1.data, iter = 1000, chains = 2, cores = 2)
print(fit.m1.1, probs = c(0.10, 0.5, 0.9))
```

Now we consider a simpler model with a different set of priors. We consider only one group which has 50 young turtles. The probability of obtaining a female of 0.3. 
We have individual-level data. (As we don't have individual-level covariates, doing a bernoulli or binomial model is very much similar, but I am changing things just for fun and exercise here. Also to see if some codes are much faster). 

```{r simulating-data-2}
Y=rbinom(50,1,0.3)
m2.data <- list(N = 50, y = Y)

```

And now we fit the model

```{r stan-model-2}
m1.2 = stan_model("m1.2.stan")
fit.m1.2 <- sampling(m1.2, data = m2.data, iter = 1000, chains = 2, cores = 2)
print(fit.m1.2, probs = c(0.10, 0.5, 0.9))

```


## Playing with priors

### Why Beta? 

Why did I choose the Beta distribution for the prior of $p$? In short, this is the *conjugate prior distribution*, some sort of canonical choice. In the past, choosing conjugate priors used to speed up monumentously the computations. Nowadays this is less true but still: 
- A conjugate prior is a prior whose probability distribution is also the distribution of the posterior. 
In other words, here if we have a Beta prior we get a Beta posterior, and if we iterate the data assimilation process 10 000 times it will still be Beta. 

[For more info and formulas](https://en.wikipedia.org/wiki/Conjugate_prior)

### What shape? 

```{r shape-beta}
curve(dbeta(x,2,2))
curve(dbeta(x,0.5,0.5),add=T,col="red")
```

Obivously red would make little sense here. What would it imply? 

```{r modified-priors}
m1.3 = stan_model("m1.3.stan")
fit.m1.3 <- sampling(m1.3, data = m2.data, iter = 1000, chains = 2, cores = 2)
print(fit.m1.3, probs = c(0.10, 0.5, 0.9))
```

Still works well but the prior is bringing no information or even poor information. 

### The consequence of too flat priors (esp. when messing with link functions)

```{stan output.var="m1.5"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  vector[N] temp;                                  // temperature
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
}
parameters {                                       // unobserved variables
  real mu_temp;
  real gamma; 
}
model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(gamma*(temp[k]-mu_temp)));       // likelihood
  //y[k] ~ bernoulli_logit(alpha+beta*temp[k]);       // likelihood -- does not work because not bernoulli of course!
  }
  mu_temp ~ normal(2, 100);        // prior of the mean temp
  gamma ~ normal(1, 100);          // prior of the slope
}
```

Let us propagate the uncertainty by visualizing the transformed parameter. We simulate according to the prior

```{r simulating-prior-and-transformed-quantities}
mu_temp = rnorm(100,2, 100) # prior of the mean temp
gamma = rnorm(100,1, 100)   # prior on the slope
x=seq(min(temperature),max(temperature),by=0.01)
par(mfrow=c(1,2))
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-0.3*(x-mu_temp[kprior])) ) 
  lines(x,prob,type="l",col="blue",add=TRUE)}
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-gamma[kprior]*(x-mean(temperature))) ) 
  lines(x,prob,type="l",col="blue",add=TRUE)}
### We get either 0 or 1. 

### Better priors
mu_temp = rnorm(100,2, 1) # prior of the mean temp
gamma = rnorm(100,1, 1)   # prior on the slope
par(mfrow=c(1,2))
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-0.3*(x-mu_temp[kprior])) ) 
  lines(x,prob,type="l",col="blue",add=TRUE)}
plot(0, bty = 'n', pch = '', ylab = "Pr(female)", xlab = "Temperature",ylim=c(0,1))
for (kprior in 1:100) {
  prob = 1/(1+exp(-abs(gamma[kprior])*(x-mean(temperature))) ) 
  lines(x,prob,type="l",col="blue",add=TRUE)}

```


### Weakly informative priors

[What are they? Why use them? A more detailed explanation](https://onlinelibrary.wiley.com/doi/10.1111/oik.05985)

## A real data example: eagles

As suggested by McElreath p. 330 of his book, records of (160!) salmon-pirating attempts by one Bald eagle on another Bald eagle (not always the same!). NB: I haven't implemented the quadratic approx. suggested by McElreath though that sounds like an excellent exercise. 

```{r eagles-exploring data}
?eagles
data(eagles) ### Explanatory variables
#P
#Size of pirating eagle (L = large, S = small).
#A
#Age of pirating eagle (I = immature, A = adult).
#V
#Size of victim eagle (L = large, S = small).
eagles.glm <- glm(cbind(y, n - y) ~ P*A + V, data = eagles,
                  family = binomial) ##classic frequentist modelling
# Small and immature birds capture less, small birds are more likely to have salmon stolen. So far so good. 
m2.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,P=as.numeric(eagles$P)-1,A=as.numeric(eagles$A)-1,V=as.numeric(eagles$V)-1)
# m2.data = list(N=nrow(eagles),y=eagles$y,z=eagles$n,P=eagles$P,A=eagles$A,V=eagles$V)

```

We're in a similar situation to the turtles sex, but with categorical explanatory variables

```{stan output.var="m2.1"}
data {                                             // observed variables 
  int<lower=1> N;                                  // number of observations
  vector[N] temp;                                  // temperature
  int<lower=0,upper=50> y[N];                      // response variable
  int<lower=0,upper=50> z[N];                      // sample size per group  
  int<lower=0,upper=1>  A[N];
  int<lower=0,upper=1>  P[N];
  int<lower=0,upper=1>  V[N];
  
}

parameters {                                       // unobserved variables
  real alpha;
  real beta_P; 
  real beta_V; 
  real beta_A; 
}

model {
  for (k in 1:N){
  y[k] ~ binomial(z[k], inv_logit(alpha + beta_P*P[k] + beta_V*V[k] +beta_A*A[k]) );       // likelihood
  }
  alpha ~ normal(0, 10);        // prior of the mean temp
  beta_P ~ normal(0, 5);          // prior of the slope
  beta_V ~ normal(0, 5);          // prior of the slope
  beta_A ~ normal(0, 5);          // prior of the slope
}

```

Fitting the model

```{r print_m2.1}
fit.m2.1 <- sampling(m2.1, data = m2.data, iter = 1000, chains = 2, cores = 2)
print(fit.m2.1, probs = c(0.10, 0.5, 0.9))
```

What do we think of these priors? 


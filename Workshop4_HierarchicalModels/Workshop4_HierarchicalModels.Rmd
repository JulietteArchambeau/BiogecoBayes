---
title: 'Workshop 4: Hierarchical Models'
author: "Juliette Archambeau, Sylvain Schmitt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    # code_fold: hide
    toc: true
    toc_depth: 5
    toc_float:
       collapsed: false
    number_sections: true
    theme: paper
    highlight: textmate
---


<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<style type="text/css">
body{ /* Normal  */
      font-size: 16px;
  }
div.main-container {
  max-width: 2000px;
  margin-left: auto;
  margin-right: auto;
}
</style>


```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 600px;
}
```

```{r setup, include=FALSE}
library(knitr)
options(width = 300)
knitr::opts_chunk$set(fig.width = 8, fig.height = 5, cache = TRUE)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(gridExtra)
library(grid)
library(ggplot2)
color_scheme_set("green")
library(tibble)
library(cowplot)
library(rethinking)
library(loo)
library(dplyr)
library(tidyr)
```



# **The data**

```{r loadData}
data <- readRDS(file="../data/sub_portugal_5provs_4blocks.rds") %>% 
  mutate_at(c("block", "prov", "clon", "tree"), as.factor) %>% # formatting data
  mutate(age.sc = as.vector(scale(age, center = F))) # as age is definite on R+ I would only reduce it..
  # mutate(age.sc = as.vector(scale(age))) # mean centering age
```

The dataset includes:

* Height data from a provenance trial (in Portugal) of maritime pine saplings. 
* Randomized block design. Here I selected 5 provenances and 4 blocks. 
* Saplings have different ages: 11, 15, 20 and 27 month old.

```{r expDesign}
table(data$prov,data$block) %>% kable(caption = "Provenance against block number.")
table(data$prov,as.factor(data$age)) %>% kable(caption = "Provenance against age (in months).")
```

```{r heightVsAge, message = F, warning = F, fig.cap="Height versus age."}
ggplot(data, aes(x=height)) + 
  geom_histogram(color="darkblue", fill="lightblue") + 
  theme_bw()

ggplot(data, aes(x=height, color=as.factor(age))) + 
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_wrap(~as.factor(age)) + 
  theme(legend.position = "none")

plot_grid(ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))

plot_grid(ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))
```

```{r heightProvBlock, message = F, warning = F, fig.cap="Height distribution by Provenance and Block.", fig.width=14,fig.height=8}
ggplot(data, aes(x=height, color=block)) +
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_grid(prov ~block, labeller = label_both) + 
  theme(legend.position = "none")
```



# **"Fixed effects" models**

> Dummy variables for each level = Regularized intercepts, because we use weakly informative priors. But no information shared between intercepts. (See P299 in Statistical Rethinking of McElreath)

First, we are going to try three different likehoods: the normal distribution, the normal distribution with a log-transformed response variable and a lognormal distribution.

## Different distributions

### Normal distribution **mN**

> Mathematical model

\begin{equation}
  \begin{aligned}
    h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}


> Data in a list.

```{r DatalistmN}
data.list <- list(N=length(data$height),              # Number of observations
                  y=data$height,                      # Response variables
                  age=data$age.sc,                    # Tree age
                  nprov=length(unique(data$prov)),    # Number of provenances
                  nblock=length(unique(data$block)),  # Number of blocks
                  prov=as.numeric(data$prov),         # Provenances
                  bloc=as.numeric(data$block))        # Blocks
```


```{r SamplingmN}
mN = stan_model("mN.stan") 
fit.mN <- sampling(mN, data = data.list, iter = 2000, chains = 2, cores = 2) 
broom::tidyMCMC(fit.mN, 
                pars = c("beta_age","beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mN with a normal distribution")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmN}
ppc_dens_overlay(y = data$height,
                 as.matrix(fit.mN, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25),
        legend.title=element_text(size=18),
        axis.text = element_text(size=18),
        legend.position = c(0.8,0.6))
```

- "In the plot above, the dark line is the **distribution of the observed outcomes** $y$ and each of the 50 lighter lines is the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) of one of the **replications of $y$ from the posterior predictive distribution** (i.e., one of the rows in yrep)." From [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html).

- This plot makes it easy to see that this model poorly fits the data. We can probably do better...



### Normal distribution with log(y) **mNlogy**

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}

```{r DatalistmNlogy}
data.list.mNlogy <- list(N=length(data$height),              # Number of observations
                  y=log(data$height),                      # log(response variable)
                  age=data$age.sc,                         # Tree age
                  nprov=length(unique(data$prov)),         # Number of provenances
                  nblock=length(unique(data$block)),       # Number of blocks
                  prov=as.numeric(data$prov),              # Provenances
                  bloc=as.numeric(data$block))             # Blocks
```

> Same stan code as the previous model!

```{r SamplingmNlogy, message = FALSE}
mNlogy = stan_model("mNlogy.stan")
fit.mNlogy <- sampling(mNlogy, data = data.list.mNlogy, iter = 2000, chains = 2, 
                       control=list(max_treedepth=14), cores = 2, save_warmup = F)
broom::tidyMCMC(fit.mNlogy, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogy with a normal distribution and a log-transformed response variable")
```

- There may be a warning about effective sample size. This warning should disappear if we increase the number of iterations to 2,500.

- We have to increase the maximum treedepth: See the [Brief Guide to Stan’s Warnings](https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded)

- lp has largely increased. 

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmNlogy}
y_rep <- as.matrix(fit.mNlogy, pars = "y_rep")
ppc_dens_overlay(y =log(data$height),
                 as.matrix(fit.mNlogy, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18), 
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

> A better fit than mN.


### LogNormal distribution **mLogNR** 

> Mathematical model

\begin{equation}
  \begin{aligned}
    h_{i} & \sim \text{LogNormal}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}


We are going to use the same data list as in the first model **mN**. 


```{r SamplingmLogNR}
mLogNR = stan_model("mLogNR.stan") 
fit.mLogNR <- sampling(mLogNR, data = data.list, iter = 2000, 
                       chains = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mLogNR, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mLogNR with a LogNormal distribution on R")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmLogNR}
ppc_dens_overlay(y = data$height,
                 as.matrix(fit.mLogNR, pars = "y_rep")[1:50, ])  + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```


## More informative priors

In the previous models, our priors are very weakly informative. We can use a little more informative priors, that can help convergence and decrease the running time. Belox, we refit the model **mNlogy** (normal distribution with log(y)) with more informative priors.

> **Variance priors**

[Prior recommendations for scale parameters in hierarchical models](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#prior-for-scale-parameters-in-hierarchical-models)


- Very weakly informative prior:  $\sigma \sim \text{HalfCauchy}(0,25)$. From Gelman (2006): 8-schools example (p430). And [here](http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf).

- Weakly informative prior: 
    
    - $\sigma \sim \text{HalfCauchy}(0,1)$ (McElreath, First version) $\sigma \sim \text{HalfCauchy}(0,5)$ ([Betancourt in 8-schools example](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html))

    - $\sigma \sim \text{exponential}(1)$ (McElreath, Second version)  or  $\sigma \sim \text{exponential}(0.1)$      

    - $\sigma \sim \text{LogNormal}(0,1)$ (McElreath, Second version)        

- More informative prior : $\sigma \sim \text{HalfNormal}(0,1)$  or $\sigma \sim \text{Half-t}(3,0,1)$ 


> **Beta priors**


- More informative priors: $\beta \sim \mathcal{N}(0,1)$. 

- We can also consider that $age$ is positively associated with height so we can add some information in our model and constrain $\beta_{age}$ to positive values, such as: $\beta_{age} \sim \text{LogNormal}(0,1)$ or $\beta_{age} \sim \text{Gamma}(?,?)$. 


### Exponential sigma **mNlogySigmaPrior**

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r SamplingmNlogySigmaPrior}
mNlogySigmaPrior = stan_model("mNlogySigmaPrior.stan")
fit.mNlogySigmaPrior <- sampling(mNlogySigmaPrior, data = data.list.mNlogy, iter = 2000, chains = 2, cores = 2,
                        control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mNlogySigmaPrior, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogySigmaPrior with a normal distribution and a log-transformed response variable")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmNlogySigmaPrior}
ppc_dens_overlay(y =log(data$height),
                 as.matrix(fit.mNlogySigmaPrior, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18), 
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```


### More informative intercepts and slopes **mNlogyBetaAlphaPrior**

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,1)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,1)\\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r SamplingmNlogyBetaAlphaPrior}
mNlogyBetaAlphaPrior = stan_model("mNlogyBetaAlphaPrior.stan")
fit.mNlogyBetaAlphaPrior <- sampling(mNlogyBetaAlphaPrior, data = data.list.mNlogy, iter = 2000, chains = 2, cores = 2,
                        control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mNlogyBetaAlphaPrior, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogyBetaAlphaPrior with a normal distribution and a log-transformed response variable")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmNlogyBetaAlphaPrior}
ppc_dens_overlay(y =log(data$height),
                 as.matrix(fit.mNlogyBetaAlphaPrior, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18), 
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

Comment: The intercept parameters change between the two models (**mNlogyBetaAlphaPrior** and **mNlogySigmaPrior**).

## Vectorized models

We use again the model **mNlogy** (normal distribution with log(y) and very weakly informative priors)

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}

### Option 1

```{r Vectorize1mNlogy}
mNlogyVectorize1 = stan_model("mNlogyVectorize1.stan")
fit.mNlogyVectorize1 <- sampling(mNlogyVectorize1, data = data.list.mNlogy, iter = 2000,
                        control=list(max_treedepth=14),
                                chains = 2, cores = 2, save_warmup = F)
broom::tidyMCMC(fit.mNlogyVectorize1, 
                pars = c("beta_age", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogyVectorize1 with a normal distribution and log(y)")
```


- Still need to increase `max_treedepth` here ! 


### Option 2

```{r Vectorize2mNlogy}
mNlogyVectorize2 = stan_model("mNlogyVectorize2.stan") 
fit.mNlogyVectorize2 <- sampling(mNlogyVectorize2, data = data.list.mNlogy, iter = 2000,
                        control=list(max_treedepth=14),
                                 chains = 2, cores = 2, save_warmup = F) 
broom::tidyMCMC(fit.mNlogyVectorize2, 
                pars = c("beta_age", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogyVectorize2 with a normal distribution and log(y)")
```

What does this model have a negative `lp_` ? Longer to run also, and low number of effective sample size. Why?


> Let's compare the speed of the 5 models with a normal distribution with log(y)

```{r comparingRunningTime}
lapply(lapply(c(fit.mNlogy, fit.mNlogySigmaPrior,fit.mNlogyBetaAlphaPrior, fit.mNlogyVectorize1, fit.mNlogyVectorize2), 
       get_elapsed_time), data.frame) %>% 
  bind_rows(.id = "model") %>% 
  mutate(model = recode_factor(model, 
                               "1" = "Model mNlogy with $\\sigma \\sim \\text{HalfCauchy}(0,25)$",
                               "2" = "Model mNlogySigmaPrior with $\\sigma \\sim \\text{Exponential}(1)$",
                               "3" = "Model mNlogyBetaAlphaPrior with $\\beta$ and $\\alpha  \\sim \\mathcal{N}(0,1)$ ",
                               "4" = "Model mNlogyVectorize1",
                               "5" = "Model mNlogyVectorize2")) %>% 
  mutate(total = warmup + sample) %>% 
  arrange(total) %>% 
  kable(caption = "Model speed comparison of models with a normal distribution and a log-transformed response variable.")
```


For the subsequent models, we will use the more informative priors of **mNlogyBetaAlphaPrior**. 


# **Multilevel models with one-varying intercept**

> Adaptive regularization

Links:

- [McElreath lecture on YouTube](https://www.youtube.com/watch?v=AALYPv5xSos)

- [Bayesian Inference 2019. Ville Hyvönen & Topias Tolonen. Chapter 6 Hierarchical models](https://vioshyvo.github.io/Bayesian_inference/hierarchical-models.html)

We are going to 

## Normal distribution with log(y)

### Centered parameterization **mmNlogyC**

P357 McElreath (first version).

mmNlogyC (multi-level model with normal distribution and log(y) - centered paramerization)

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{PROV[p]}\\
    \alpha_{PROV} & \sim \mathcal{N}(\mu_{\alpha_{PROV}},\sigma_{\alpha_{PROV}})\\
    \mu_{\alpha_{PROV}} & \sim \mathcal{N}(0,1)\\
    \sigma_{\alpha_{PROV}}& \sim \text{Exponential}(1)\\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r ReducedDatalist}
data.list.reduced <- list(N=length(data$height),         # Number of observations
                  y=log(data$height),                    # Log(Response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  prov=as.numeric(data$prov))            # Provenances
```

```{r SamplingmmNlogyC}
mmNlogyC <- stan_model("mmNlogyC.stan")  
fit.mmNlogyC <- sampling(mmNlogyC, data = data.list.reduced , iter = 2000, chains = 2, 
                                 cores = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mmNlogyC,
                pars = c("beta_age", "beta_age2", "mean_alpha_prov", "sigma_alpha_prov",
                         "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "One-varying intercept model mmNlogyC with centered parameterization")
```

This model has some divergent transitions, low number of effective sample size and R-hat not exactly equal to one.

> Divergent transitions


- To avoid divergent transitions, we can increase `adapt_delta`.. 

McEleath (Second version) : "[...] the target acceptance rate is controlled by the `adapt_delta` control parameter. The default is 0.95, which means that it aims to attain a 95% acceptance rate. It tries this during the warmup phase, adjusting the step size of each leapfrog step (go back to Chapter 9 if these terms aren’t familiar). When `adapt_delta` is set high, it results in a smaller step size, which means a more accurate approximation of the curved surface. It also means more computation, which means a slower chain. Increasing adapt_delta can often, but not always, help with divergent transitions."

- You can also try to add some information in your model, for instance with $\beta_{age} \sim \text{LogNormal}(0,1)$.

- Or you can reparameterize your model with the **non-centered parameterization**. (Best option!!)



```{r PostPredictmmNlogyC}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmNlogyC, pars = "y_rep")[1:50, ]) + 
  theme_bw() +
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r caterpillarmmNlogyC,fig.width=12}
mcmc_trace(as.array(fit.mmNlogyC), pars = c("alpha_prov[3]","sigma_alpha_prov"), 
           np = nuts_params(fit.mmNlogyC)) + 
  xlab("Post-warmup iteration")
```

```{r pairsPlotsmmNlogyC,fig.width=17, fig.height=10}
mcmc_pairs(as.array(fit.mmNlogyC), np = nuts_params(fit.mmNlogyC), 
           pars = c("beta_age","beta_age2","mean_alpha_prov","sigma_alpha_prov","alpha_prov[3]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=2, div_shape = 19))
```



### Non-centered parameterization **mmNlogyNC**

Links: 

- [Stan user guide](https://mc-stan.org/docs/2_22/stan-users-guide/reparameterization-section.html)

- https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html

- https://jrnold.github.io/bugs-examples-in-stan/aspirin.html


***

From McElreath, P429 (13.4.2.) of Statistical Rethinking (second version) 


$$ \alpha \sim \mathcal{N}(\mu,\sigma)$$


is equivalent to 

\begin{equation}
  \begin{aligned}
    \alpha &= \mu + \beta\\
    \beta &\sim \mathcal{N}(0,\sigma)
  \end{aligned}
\end{equation}

is equivalent to

\begin{equation}
  \begin{aligned}
    \alpha &= \mu + z\sigma\\
    z &\sim \mathcal{N}(0,1)
  \end{aligned}
\end{equation}

No parameters are left inside the prior.

***

From [Updating: A Set of Bayesian Notes. Jeffrey B. Arnold. 20 Multilevel Models](https://jrnold.github.io/bayesian_notes/multilevel-models.html)

These are two ways of writing the same model. However, they change the parameters that the HMC algorithm is actively sampling and thus can have different sampling performance.

However, neither is universally better.

  - If much data, the non-centered parameterization works better
  - If less data, the centered parameterization works better

And there is currently no ex-ante way to know which will work better, and at what amount of “data” that the performance of one or the other is better. However, one other reason to use the centered parameterization (if it is also scaled), is that the Stan HMC implementation tends to be more efficient if all parameters are on the scale.

***

mmNlogyNC (multi-level model with normal distribution and log(y) - non-centered paramerization)

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(log(\mu_{i}),\sigma_{i})\\
    \mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{PROV[p]}\sigma_{PROV}\\
    \alpha & \sim \mathcal{N}(0,1)\\
    z_{PROV} & \sim \mathcal{N}(0,1)\\
    \sigma_{\alpha_{PROV}}& \sim \text{Exponential}(1)\\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r SamplingmmNlogyNC}
mmNlogyNC <- stan_model("mmNlogyNC.stan")
fit.mmNlogyNC <- sampling(mmNlogyNC, data = data.list.reduced, iter = 2000, chains = 2,
                          cores = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mmNlogyNC,
                pars = c("beta_age", "beta_age2", "z_prov", "sigma_prov","alpha",
                         "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "One-varying intercept model mmNlogyNC with non-centered parameterization")
```

> No more divergent transitions! But still low effective sample size. 


```{r PostPredictmmNlogyNC}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmNlogyNC, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r caterpillarmmNlogyNC, fig.width = 16}
mcmc_trace(as.array(fit.mmNlogyNC), 
           pars =c( "z_prov[3]","sigma_prov"), 
           np = nuts_params(fit.mmNlogyNC)) + 
  xlab("Post-warmup iteration")
```

```{r pairsPlotsmmNlogyNC, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmNlogyNC), np = nuts_params(fit.mmNlogyNC),
           pars = c("beta_age","beta_age2","alpha","sigma_prov","z_prov[3]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


## Lognormal distribution

### Non-centered parameterization **mmLogNnc**

***

For a lognormal distribution the non-centered parameterisation is different:

$$\alpha \sim \mathcal{logN}(log(\mu),\sigma)$$

is equivalent to

\begin{equation}
  \begin{aligned}
    \alpha &=  e^{log(\mu) + z.\sigma} \\
    z &\sim \mathcal{N}(0,1)
  \end{aligned}
\end{equation}

***

mmLogNnc (multi-level LogNormal dsitribution - non-centered paramerization)

> Mathematical model

\begin{equation}
  \begin{aligned}
    h_{i} & \sim \text{LogNormal}(log(\mu_{i}),\sigma_{i})\\
    \mu_{i} & = e^{log(\alpha) + z_{PROV[p]}\sigma_{PROV}} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} \\
    \alpha & \sim \text{LogNormal}(0,1) \\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    z_{PROV} & \sim \mathcal{N}(0,1)\\
    \sigma_{PROV} & \sim \text{Exponential}(1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}

```{r ReducedDatalistLogN}
data.list.reduced.logN <- list(N=length(data$height),         # Number of observations
                  y=data$height,                              # Response variable
                  age=data$age.sc,                            # Tree age
                  nprov=length(unique(data$prov)),            # Number of provenances
                  prov=as.numeric(data$prov))                 # Provenances
```

```{r SamplingmmLogNnc}
mmLogNnc<- stan_model("mmLogNnc.stan")
fit.mmLogNnc <- sampling(mmLogNnc, data = data.list.reduced.logN, iter = 2000, chains = 2,
                          cores = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mmLogNnc,
                pars = c("beta_age", "beta_age2", "z_prov", "sigma_prov","alpha",
                         "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "One-varying intercept model mmLogNnc with Lognormal non-centered parameterization")
```

```{r PostPredictmmLogNnc}
ppc_dens_overlay(y = data$height,
                 as.matrix(fit.mmLogNnc, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```


```{r caterpillarmmLogNnc, fig.width = 16}
mcmc_trace(as.array(fit.mmLogNnc), 
           pars =c( "z_prov[3]","sigma_prov"), 
           np = nuts_params(fit.mmLogNnc)) + 
  xlab("Post-warmup iteration")
```

```{r pairsPlotsmmLogNnc, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmLogNnc), np = nuts_params(fit.mmLogNnc),
           pars = c("beta_age","beta_age2","alpha","sigma_prov","z_prov[3]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



# **Two varying intercepts** (prov and block)

## Centered parameterization 

### Two global mean parameters **mm2NlogyCa**


\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{n}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha_{BLOCK} & \sim \mathcal{N}(\mu_{\alpha_{BLOCK}},\sigma_{\alpha_{BLOCK}})\\
\alpha_{PROV} & \sim \mathcal{N}(\mu_{\alpha_{PROV}},\sigma_{\alpha_{PROV}})\\
\mu_{\alpha_{PROV}} & \sim \mathcal{N}(0,1)\\
\mu_{\alpha_{BLOCK}}& \sim \mathcal{N}(0,1)\\
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)
\end{aligned}
\end{equation}


```{r Datalistmm2}
data.list.mm2 <- list(N=length(data$height),             # Number of observations
                  y=log(data$height),                    # log(response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  nblock=length(unique(data$block)),     # Number of blocks
                  prov=as.numeric(data$prov),            # Provenances
                  bloc=as.numeric(data$block))           # Blocks
```


```{r Samplingmm2NlogyCa}
mm2NlogyCa = stan_model("mm2NlogyCa.stan")
fit.mm2NlogyCa <- sampling(mm2NlogyCa, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyCa,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", 
                           "mean_alpha_prov","sigma_alpha_prov",
                           "mean_alpha_block","sigma_alpha_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercept model mm2NlogyCa with centered parameterization")
```

```{r PostPredictmm2NlogyCa}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyCa, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyCa, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCa), np = nuts_params(fit.mm2NlogyCa),
           pars = c("mean_alpha_prov","sigma_alpha_prov",
                    "mean_alpha_block","sigma_alpha_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

McElreath: "[...] We can’t identify a separate mean for each varying intercept type, because both intercepts are added to the same linear prediction. So it is conventional to define varying intercepts with a mean of zero, so there’s no risk of accidentally creating hard-to-identify parameters." "If you do include a mean for each cluster type, it won’t be the end of the world, however."



### One global mean parameter **mm2NlogyCb**

Same model but with one global mean parameter $\alpha$, and both of the varying intercept parameters ($\alpha_{BLOCK}$ and $\alpha_{PROV}$) are centered at zero.

\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha & \sim \mathcal{N}(0,1)\\
\alpha_{BLOCK} & \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})\\
\alpha_{PROV} & \sim \mathcal{N}(0,\sigma_{\alpha_{PROV}})\\
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)\\
\end{aligned}
\end{equation}

```{r Samplingmm2NlogyCb}
mm2NlogyCb = stan_model("mm2NlogyCb.stan")
fit.mm2NlogyCb <- sampling(mm2NlogyCb, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyCb,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", "alpha",
                           "sigma_alpha_prov","sigma_alpha_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercept model mm2NlogyCb with centered parameterization")
```




```{r PostPredictmm2NlogyCb}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyCb, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyCb, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha","sigma_alpha_prov","sigma_alpha_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

> The funnel geometry

The Hamiltonian Markov chain might explore the funnel neck less easily.

```{r pairsPlotsmm2NlogyCbFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]","sigma_alpha_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


[Betancourt](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)

## Non-centered parameterization **mm2NlogyNC**


\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{BLOCK[b]}\sigma_{BLOCK} + z_{PROV[p]}\sigma_{PROV}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha & \sim \mathcal{N}(0,1)\\
z_{BLOCK} & \sim \mathcal{N}(0,1)\\
z_{PROV} & \sim \mathcal{N}(0,1)\\
\sigma_{PROV} & \sim \text{Exponential}(1)\\
\sigma_{BLOCK} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)\\
\end{aligned}
\end{equation}



```{r Samplingmm2NlogyNC}
mm2NlogyNC = stan_model("mm2NlogyNC.stan")
fit.mm2NlogyNC <- sampling(mm2NlogyNC, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyNC,
                 pars = c("beta_age","beta_age2",
                           "z_prov","z_block", "alpha",
                           "sigma_prov","sigma_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercept model mm2NlogyNC with centered parameterization")
```


```{r PostPredictmm2NlogyNC}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyNC, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyNC, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyNC), np = nuts_params(fit.mm2NlogyNC),
           pars = c("alpha","sigma_prov","sigma_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


```{r pairsPlotsmm2NlogyNCFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyNC), np = nuts_params(fit.mm2NlogyNC),
           pars = c("z_prov[1]","z_prov[2]","z_prov[3]","z_prov[4]","z_prov[5]","sigma_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



> Increasing acceptance rate? 

```{r mm2NlogyNCAdaptDelta, fig.width = 17, fig.height = 10}
fit.mm2NlogyNC <- sampling(mm2NlogyNC, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14,adapt_delta=0.99))
mcmc_pairs(as.array(fit.mm2NlogyNC), np = nuts_params(fit.mm2NlogyNC),
           pars = c("z_prov[1]","z_prov[2]","z_prov[3]","z_prov[4]","z_prov[5]","sigma_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



# **Multilevel models with varying intercepts and slopes**

> P439 of McElreath (Second edition)

Provenances vary in their **average** height but may also have **different relationship** with age. The linear model might look like this:

$$\mu_{i} = \alpha_{PROV[p]} + \beta_{PROV[p]}age_{i} $$

Like before with the varying intercepts, we want to pool information about slopes, by estimating the variance of the slopes.

One important point however: provenances are likely to covary in their intercepts and slopes. For instance, we may hypothese that higher provenances on average are also those that grow faster. So we want a way to pool information across parameter types (intercepts and slopes).

"How should the robot[=the model] pool information across intercepts and slopes? By modeling the **joint population of intercepts and slopes**, which means by **modeling their covariance**. In conventional multilevel models, the device that makes this possible is a **joint multivariate Gaussian distribution** for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension)."

"**Why Gaussian?** There is no reason the multivariate distribution of intercepts and slopes must be Gaussian. But there are both practical and epistemological justifications. On the practical side, there aren’t many multivariate distributions that are easy to work with. The only common ones are multivariate Gaussian and multivariate Student (or “t”) distributions. On the epistemological side, if all we want to say about these intercepts and slopes is their means, variances, and covariances, then the maximum entropy distribution is multivariate Gaussian."


## Centered parameterization **mmCovNlogyClkj2**


\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + \alpha_{BLOCK[b]} + \alpha_{PROV[p]} + \beta_{PROV[p]}age_{i} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} \\[4pt]

\begin{bmatrix}  \alpha_{PROV} \\  \beta_{PROV}  
    \end{bmatrix} & \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{S} \right) \\[4pt]

\mathbf{S} & = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} 
          \mathbf{R}
          \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} \\[4pt]
          
\alpha & \sim \text{LogNormal}(0,1)\\[4pt] 
\beta_{age} & \sim \text{LogNormal}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]


\alpha_{BLOCK} & \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})\\[4pt]

\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\mathbf{R} & \sim \text{LKJcorr(2)}

\end{aligned}
\end{equation}

***

> Details from McEleath's book, P445, 14.1.3. The varying slopes model.


- $\begin{bmatrix}  \alpha_{PROV} \\  \beta_{PROV} \end{bmatrix} \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{S} \right)$ 

This line states that each provenance has an intercept $\alpha_{PROV}$ and slope $\beta_{PROV}$ with a prior distribution defined by the two-dimensional Gaussian distribution with mean 0 and covariance matrix $\mathbf{S}$. This statement of prior will adaptively regularize the individual intercepts, slopes, and the correlation among them.

- $\mathbf{S} = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} \mathbf{R} \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix}$. 

$\mathbf{S}$ is the covariance matrix and can be written by factoring it into separate standard deviations, $\sigma_{\alpha_{PROV}}$ and $\sigma_{\beta_{PROV}}$, and a correlation matrix $\mathbf{R}$. $\mathbf{R}$ can be written as follows:
$$\mathbf{R} = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$$
where $\rho$ is the correlation between intercepts and slopes.

- The rest of the model just defines fixed priors. The prior of $\mathbf{R}$ is the LKJcorr prior, with only one parameter to define it. In larger matrices, with additional varying slopes, the same LKJcorr prior will work. What LKJcorr(2) does is define a weakly informative prior on $\rho$ that is skeptical of extreme correlations near −1 or 1. You can think of it as a **regularizing prior for correlations**. This distribution has a single parameter, $\eta$, that controls how skeptical the prior is of large correlations in the matrix. When we use LKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, such as the 2 we used above, then extreme correlations are less likely.

```{r VizualizePriorLKJcorr,fig.width=4,fig.height=3}
Rho2 <- rlkjcorr( 1e4 , K=2 , eta=2 )
Rho1 <- rlkjcorr( 1e4 , K=2 , eta=1 )
Rho4 <- rlkjcorr( 1e4 , K=2 , eta=4 )
plot_grid(dens(Rho1[,1,2],xlim=c(-1,1),xlab="correlation" ,ylim=c(0,1.2),main="eta=1"),
         dens(Rho2[,1,2] , xlab="correlation" ,xlim=c(-1,1),ylim=c(0,1.2),main="eta=2"),
         dens(Rho4[,1,2],xlim=c(-1,1),xlab="correlation" ,ylim=c(0,1.2),main="eta=4"))
```


> So, how is it in Stan? 

The following model is based on: [Stan code of Statistical Rethinking. 13.3 Example: cross-classified chimpanzees with varying slopes](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd). But, take care, in this example, some priors are missing!


```{r DatalistmmCov}
data.list.mmCov <- list(N=length(data$height),           # Number of observations
                  y=log(data$height),                    # log(response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  nblock=length(unique(data$block)),     # Number of blocks
                  prov=as.numeric(data$prov),            # Provenances
                  bloc=as.numeric(data$block))           # Blocks
```


```{r SamplingmmCovNlogyClkj2}
mmCovNlogyClkj2 = stan_model("mmCovNlogyClkj2.stan")
fit.mmCovNlogyClkj2 <- sampling(mmCovNlogyClkj2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovNlogyClkj2,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyClkj2 with varying intercepts and slopes with centered parameterization")
```


```{r PostPredictmmCovNlogyClkj2}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovNlogyClkj2, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyClkj2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyClkj2), np = nuts_params(fit.mmCovNlogyClkj2),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmmCovNlogyClkj2AlphaBeta, fig.width = 17, fig.height = 15}
mcmc_pairs(as.array(fit.mmCovNlogyClkj2), np = nuts_params(fit.mmCovNlogyClkj2),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]",
                    "beta_prov[1]","beta_prov[2]","beta_prov[3]","beta_prov[4]","beta_prov[5]"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



Here the prior of the correlation matrix  $\mathbf{R}$ was $\text{LKJcorr(2)}$. We can try a more regularizing prior, which penalizes more the large correlations, for instance $\text{LKJcorr(4)}$. I saw that in `brms`, a package based on stan (more  usser-firendly for those who don't want to code their models in Stan) that they use $\text{LKJcorr(4)}$.


```{r SamplingmmCovNlogyClkj4}
mmCovNlogyClkj4 = stan_model("mmCovNlogyClkj4.stan")
fit.mmCovNlogyClkj4 <- sampling(mmCovNlogyClkj4, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovNlogyClkj4,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyClkj4 with varying intercepts and slopes with centered parameterization")
```


```{r PostPredictmmCovNlogyClkj4}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovNlogyClkj4, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyClkj4, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyClkj4), np = nuts_params(fit.mmCovNlogyClkj4),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


> We will use $\text{LKJcorr(4)}$ in all the subsequent models. 


## Non-centered parameterization 

### Version 1 **mmCovlogyNC1**

> P408 McElreath Statistical rethinking (First version)

> [stan code here](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd#L580)



\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + z_{\alpha_{BLOCK[b]}}\sigma_{\alpha_{BLOCK}} + z_{\alpha_{PROV[p]}}\sigma_{\alpha_{PROV}} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{\beta_{PROV[p]}}\sigma_{\beta_{PROV}}age_{i} \\[4pt]

\begin{bmatrix}  z_{\alpha_{PROV}} \\  z_{\beta_{PROV}} 
    \end{bmatrix} & \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{R} \right) \\[4pt]

          
          
\beta_{age} & \sim \mathcal{N}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]
\alpha & \sim \mathcal{N}(0,1)\\[4pt]

z_{\alpha_{BLOCK}} & \sim \mathcal{N}(0,1)\\[4pt]
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]

\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]


\mathbf{R}& \sim \text{LKJcorr(4)}

\end{aligned}
\end{equation}

Based on McEleath: "**Notice that each varying intercept and slope is multiplied by its corresponding scale parameter**, one of the standard deviations that ordinarily appears inside the multivariate Gaussian prior for these effects. This just undoes the standardization that is imposed in the prior. Inside the priors, there are no covariance matrices now. There are just correlation matrices, since the scale of each dimension has been migrated to the linear model. So these adaptive priors are essentially distributions of correlated z-scores."

```{r SamplingmmCovNlogyNC1}
mmCovlogyNC1 = stan_model("mmCovNlogyNC1.stan")
fit.mmCovlogyNC1 <- sampling(mmCovlogyNC1, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovlogyNC1,
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block","z_alpha_prov",
                          "z_beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovlogyNC1 with varying intercepts and slopes with non-centered parameterization")
```


```{r PostPredictmmCovNlogyNC1}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovlogyNC1, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyNC1, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovlogyNC1), np = nuts_params(fit.mmCovlogyNC1),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```






> Comment: The correlation is still in the matrix, **how can we take the correlation matrix out of the prior??**



### Cholesky decomposition version 


McElreath (p457, Second version): "It is possible to further extend this non-centering strategy of taking parameters out of the prior. All that is left in the priors above is the correlation matrix, $\mathbf{R}$ . These too can be extracted by using a **Cholesky decomposition** of the correlation matrix. A Cholesky decomposition $\mathbf{L}$ is a way to represent a square, symmetric matrix like a correlation matrix $\mathbf{R}$ such that $\mathbf{R} = \mathbf{L}\mathbf{L}^\intercal$ . It is a marvelous fact that you can multiply $\mathbf{L}$ by a matrix of uncorrelated samples (z-scores) and end up with a matrix of correlated samples (the varying effects). This is the trick that lets us take the covariance matrix out of the prior. We just sample a matrix of uncorrelated z-scores and then multiply those by the Cholesky factor and the standard deviations to get the varying effects with the correct scale and correlation. It would be magic, except that it is just algebra.""

> Let's look at three ways to write the same model. I think they are equivalent, it's just a question of writing.

#### Option 1 **mmCovNlogyNCcholdec1**

> Following McEleath P457 Second version of Statistical Rethinking.

```{r SamplingmmCovNlogyNCcholdec1}
mmCovNlogyNCcholdec1= stan_model("mmCovNlogyNCcholdec1.stan")
fit.mmCovNlogyNCcholdec1 <- sampling(mmCovNlogyNCcholdec1, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec1, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "v_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec1 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec1, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec1), np = nuts_params(fit.mmCovNlogyNCcholdec1),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


#### Option 2 **mmCovNlogyNCcholdec2**

> [stan code](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd#L680)

```{r SamplingmmCovNlogyNCcholdec2}
mmCovNlogyNCcholdec2= stan_model("mmCovNlogyNCcholdec2.stan")
fit.mmCovNlogyNCcholdec2 <- sampling(mmCovNlogyNCcholdec2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec2, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "alpha_prov","beta_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec2 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec2), np = nuts_params(fit.mmCovNlogyNCcholdec2),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


#### Option 3 **mmCovNlogyNCcholdec3**

> Similar to [Sorensen et al. 2016. Listing 8.](http://jakewestfall.org/misc/SorensenEtAl.pdf)

```{r SamplingmmCovNlogyNCcholdec3}
mmCovNlogyNCcholdec3= stan_model("mmCovNlogyNCcholdec3.stan")
fit.mmCovNlogyNCcholdec3 <- sampling(mmCovNlogyNCcholdec3, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec3, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "v_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec3 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec3, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec3), np = nuts_params(fit.mmCovNlogyNCcholdec3),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

> Effective sample size comparison


```{r nEffCompare}
tibble("mmCovNlogyClkj4"=summary(fit.mmCovNlogyClkj4)$summary[1:25,"n_eff"],
"mmCovlogyNC1"=summary(fit.mmCovlogyNC1)$summary[1:25,"n_eff"],
"mmCovNlogyNCcholdec1"=summary(fit.mmCovNlogyNCcholdec1)$summary[1:25,"n_eff"]) %>% 
  tidyr::pivot_longer(everything(),names_to = "model", values_to = "n_eff") %>% 
  ggplot(aes(x=model, y=n_eff)) + 
  geom_boxplot() + theme_bw()
```


> Running time

```{r comparingRunningTimeMM}
lapply(lapply(c(fit.mmCovNlogyClkj4, fit.mmCovlogyNC1,fit.mmCovNlogyNCcholdec1), 
       get_elapsed_time), data.frame) %>% 
  bind_rows(.id = "model") %>% 
  mutate(model = recode_factor(model, 
                               "1" = "Model mmCovNlogyClkj4 - centered",
                               "2" = "Model mmCovlogyNC1 - non-centered",
                               "3" = "Model mmCovNlogyNCcholdec1 - non-centered with the Cholesky decomposition")) %>% 
  mutate(total = warmup + sample) %>% 
  arrange(total) %>% 
  kable(caption = "Model speed comparison of multilevels models with covarying intercepts and slopes")
```

`r knitr::opts_chunk$set(eval = F)`

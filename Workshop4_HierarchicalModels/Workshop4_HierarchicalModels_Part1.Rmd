---
title: 'Workshop 4: Hierarchical Models - Part 1'
author: "Juliette Archambeau, Sylvain Schmitt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    # code_fold: hide
    toc: true
    toc_depth: 5
    toc_float:
       collapsed: false
    number_sections: true
    theme: paper
    highlight: textmate
---


<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<style type="text/css">
body{ /* Normal  */
      font-size: 16px;
  }
div.main-container {
  max-width: 2000px;
  margin-left: auto;
  margin-right: auto;
}
</style>


```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 600px;
}
```

```{r setup, include=FALSE}
library(knitr)
options(width = 300)
knitr::opts_chunk$set(fig.width = 8, fig.height = 5, cache = TRUE)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(gridExtra)
library(grid)
library(ggplot2)
color_scheme_set("green")
library(tibble)
library(cowplot)
library(rethinking)
library(loo)
library(dplyr)
library(tidyr)
```



# **The data**

```{r loadData}
data <- readRDS(file="../data/sub_portugal_5provs_4blocks.rds") %>% 
  mutate_at(c("block", "prov", "clon", "tree"), as.factor) %>% # formatting data
  mutate(age.sc = as.vector(scale(age, center = F))) # as age is definite on R+ I would only reduce it..
  # mutate(age.sc = as.vector(scale(age))) # mean centering age
```

The dataset includes:

* Height data from a provenance trial (in Portugal) of maritime pine saplings. 
* Randomized block design. Here I selected 5 provenances and 4 blocks. 
* Saplings have different ages: 11, 15, 20 and 27 month old.

```{r expDesign}
table(data$prov,data$block) %>% kable(caption = "Provenance against block number.")
table(data$prov,as.factor(data$age)) %>% kable(caption = "Provenance against age (in months).")
```

```{r heightVsAge, message = F, warning = F, fig.cap="Height versus age."}
ggplot(data, aes(x=height)) + 
  geom_histogram(color="darkblue", fill="lightblue") + 
  theme_bw()

ggplot(data, aes(x=height, color=as.factor(age))) + 
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_wrap(~as.factor(age)) + 
  theme(legend.position = "none")

plot_grid(ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))

plot_grid(ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))
```

```{r heightProvBlock, message = F, warning = F, fig.cap="Height distribution by Provenance and Block.", fig.width=14,fig.height=8}
ggplot(data, aes(x=height, color=block)) +
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_grid(prov ~block, labeller = label_both) + 
  theme(legend.position = "none")
```



# **"Fixed effects" models**

> Dummy variables for each level = Regularized intercepts, because we use weakly informative priors. But no information shared between intercepts. (See P299 in Statistical Rethinking of McElreath)

First, we are going to try three different likehoods: the normal distribution, the normal distribution with a log-transformed response variable and a lognormal distribution.

## Different distributions

### Normal distribution **mN**

> Mathematical model

\begin{equation}
  \begin{aligned}
    h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}


> Data in a list.

```{r DatalistmN}
data.list <- list(N=length(data$height),              # Number of observations
                  y=data$height,                      # Response variables
                  age=data$age.sc,                    # Tree age
                  nprov=length(unique(data$prov)),    # Number of provenances
                  nblock=length(unique(data$block)),  # Number of blocks
                  prov=as.numeric(data$prov),         # Provenances
                  bloc=as.numeric(data$block))        # Blocks
```


```{r SamplingmN}
mN = stan_model("mN.stan") 
fit.mN <- sampling(mN, data = data.list, iter = 2000, chains = 2, cores = 2) 
broom::tidyMCMC(fit.mN, 
                pars = c("beta_age","beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mN with a normal distribution")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmN}
ppc_dens_overlay(y = data$height,
                 as.matrix(fit.mN, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25),
        legend.title=element_text(size=18),
        axis.text = element_text(size=18),
        legend.position = c(0.8,0.6))
```

- "In the plot above, the dark line is the **distribution of the observed outcomes** $y$ and each of the 50 lighter lines is the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) of one of the **replications of $y$ from the posterior predictive distribution** (i.e., one of the rows in yrep)." From [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html).

- This plot makes it easy to see that this model poorly fits the data. We can probably do better...



### Normal distribution with log(y) **mNlogy**

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}

```{r DatalistmNlogy}
data.list.mNlogy <- list(N=length(data$height),              # Number of observations
                  y=log(data$height),                      # log(response variable)
                  age=data$age.sc,                         # Tree age
                  nprov=length(unique(data$prov)),         # Number of provenances
                  nblock=length(unique(data$block)),       # Number of blocks
                  prov=as.numeric(data$prov),              # Provenances
                  bloc=as.numeric(data$block))             # Blocks
```

> Same stan code as the previous model!

```{r SamplingmNlogy, message = FALSE}
mNlogy = stan_model("mNlogy.stan")
fit.mNlogy <- sampling(mNlogy, data = data.list.mNlogy, iter = 2000, chains = 2, 
                       control=list(max_treedepth=14), cores = 2, save_warmup = F)
broom::tidyMCMC(fit.mNlogy, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogy with a normal distribution and a log-transformed response variable")
```

- There may be a warning about effective sample size. This warning should disappear if we increase the number of iterations to 2,500.

- We have to increase the maximum treedepth: See the [Brief Guide to Stan’s Warnings](https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded)

- lp has largely increased. 

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmNlogy}
y_rep <- as.matrix(fit.mNlogy, pars = "y_rep")
ppc_dens_overlay(y =log(data$height),
                 as.matrix(fit.mNlogy, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18), 
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

> A better fit than mN.


### LogNormal distribution **mLogNR** 

> Mathematical model

\begin{equation}
  \begin{aligned}
    h_{i} & \sim \text{LogNormal}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}


We are going to use the same data list as in the first model **mN**. 


```{r SamplingmLogNR}
mLogNR = stan_model("mLogNR.stan") 
fit.mLogNR <- sampling(mLogNR, data = data.list, iter = 2000, 
                       chains = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mLogNR, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mLogNR with a LogNormal distribution on R")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmLogNR}
ppc_dens_overlay(y = data$height,
                 as.matrix(fit.mLogNR, pars = "y_rep")[1:50, ])  + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```


## More informative priors

In the previous models, our priors are very weakly informative. We can use a little more informative priors, that can help convergence and decrease the running time. Belox, we refit the model **mNlogy** (normal distribution with log(y)) with more informative priors.

> **Variance priors**

[Prior recommendations for scale parameters in hierarchical models](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#prior-for-scale-parameters-in-hierarchical-models)


- Very weakly informative prior:  $\sigma \sim \text{HalfCauchy}(0,25)$. From Gelman (2006): 8-schools example (p430). And [here](http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf).

- Weakly informative prior: 
    
    - $\sigma \sim \text{HalfCauchy}(0,1)$ (McElreath, First version) $\sigma \sim \text{HalfCauchy}(0,5)$ ([Betancourt in 8-schools example](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html))

    - $\sigma \sim \text{exponential}(1)$ (McElreath, Second version)  or  $\sigma \sim \text{exponential}(0.1)$      

    - $\sigma \sim \text{LogNormal}(0,1)$ (McElreath, Second version)        

- More informative prior : $\sigma \sim \text{HalfNormal}(0,1)$  or $\sigma \sim \text{Half-t}(3,0,1)$ 


> **Beta priors**


- More informative priors: $\beta \sim \mathcal{N}(0,1)$. 

- We can also consider that $age$ is positively associated with height so we can add some information in our model and constrain $\beta_{age}$ to positive values, such as: $\beta_{age} \sim \text{LogNormal}(0,1)$ or $\beta_{age} \sim \text{Gamma}(?,?)$. 


### Exponential sigma **mNlogySigmaPrior**

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r SamplingmNlogySigmaPrior}
mNlogySigmaPrior = stan_model("mNlogySigmaPrior.stan")
fit.mNlogySigmaPrior <- sampling(mNlogySigmaPrior, data = data.list.mNlogy, iter = 2000, chains = 2, cores = 2,
                        control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mNlogySigmaPrior, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogySigmaPrior with a normal distribution and a log-transformed response variable")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmNlogySigmaPrior}
ppc_dens_overlay(y =log(data$height),
                 as.matrix(fit.mNlogySigmaPrior, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18), 
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```


### More informative intercepts and slopes **mNlogyBetaAlphaPrior**

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,1)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,1)\\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r SamplingmNlogyBetaAlphaPrior}
mNlogyBetaAlphaPrior = stan_model("mNlogyBetaAlphaPrior.stan")
fit.mNlogyBetaAlphaPrior <- sampling(mNlogyBetaAlphaPrior, data = data.list.mNlogy, iter = 2000, chains = 2, cores = 2,
                        control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mNlogyBetaAlphaPrior, 
                pars = c("beta_age", "beta_age2", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogyBetaAlphaPrior with a normal distribution and a log-transformed response variable")
```

> Comparison of the distribution of $y$ and the posterior predictive distributions (from `yrep` matrix).

```{r PostPredictmNlogyBetaAlphaPrior}
ppc_dens_overlay(y =log(data$height),
                 as.matrix(fit.mNlogyBetaAlphaPrior, pars = "y_rep")[1:50, ]) + 
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18), 
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

Comment: The intercept parameters change between the two models (**mNlogyBetaAlphaPrior** and **mNlogySigmaPrior**).

## Vectorized models

We use again the model **mNlogy** (normal distribution with log(y) and very weakly informative priors)

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
    \alpha_{BLOCK} & \sim \mathcal{N}(0,10)\\
    \alpha_{PROV} & \sim \mathcal{N}(0,10)\\
    \beta_{age} & \sim \mathcal{N}(0,10) \\
    \beta_{age2} & \sim \mathcal{N}(0,10)\\
    \sigma & \sim \text{HalfCauchy}(0,25)
  \end{aligned}
\end{equation}

### Option 1

```{r Vectorize1mNlogy}
mNlogyVectorize1 = stan_model("mNlogyVectorize1.stan") 
fit.mNlogyVectorize1 <- sampling(mNlogyVectorize1, data = data.list.mNlogy, iter = 2000,
                        control=list(max_treedepth=14),
                                chains = 2, cores = 2, save_warmup = F)
broom::tidyMCMC(fit.mNlogyVectorize1, 
                pars = c("beta_age", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogyVectorize1 with a normal distribution and log(y)")
```


- Still need to increase `max_treedepth` here ! 


### Option 2

```{r Vectorize2mNlogy}
mNlogyVectorize2 = stan_model("mNlogyVectorize2.stan")
fit.mNlogyVectorize2 <- sampling(mNlogyVectorize2, data = data.list.mNlogy, iter = 2000,
                        control=list(max_treedepth=14),
                                 chains = 2, cores = 2, save_warmup = F) 
broom::tidyMCMC(fit.mNlogyVectorize2, 
                pars = c("beta_age", "alpha_prov", "alpha_block", "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>% 
  kable(caption = "Model mNlogyVectorize2 with a normal distribution and log(y)")
```


> Let's compare the speed of the 5 models with a normal distribution with log(y)

```{r comparingRunningTime}
lapply(lapply(c(fit.mNlogy, fit.mNlogySigmaPrior,fit.mNlogyBetaAlphaPrior, fit.mNlogyVectorize1, fit.mNlogyVectorize2), 
       get_elapsed_time), data.frame) %>% 
  bind_rows(.id = "model") %>%
  mutate(model = recode_factor(model,
                               "1" = "Model mNlogy with $\\sigma \\sim \\text{HalfCauchy}(0,25)$",
                               "2" = "Model mNlogySigmaPrior with $\\sigma \\sim \\text{Exponential}(1)$",
                               "3" = "Model mNlogyBetaAlphaPrior with $\\beta$ and $\\alpha  \\sim \\mathcal{N}(0,1)$ ",
                               "4" = "Model mNlogyVectorize1",
                               "5" = "Model mNlogyVectorize2")) %>% 
  mutate(total = warmup + sample) %>% 
  arrange(total) %>% 
  kable(caption = "Model speed comparison of models with a normal distribution and a log-transformed response variable.")
```


For the subsequent models, we will use the more informative priors of **mNlogyBetaAlphaPrior**. 


# **Multilevel models with one-varying intercept**

> Adaptive regularization

Links:

- [McElreath lecture on YouTube](https://www.youtube.com/watch?v=AALYPv5xSos)

- [Bayesian Inference 2019. Ville Hyvönen & Topias Tolonen. Chapter 6 Hierarchical models](https://vioshyvo.github.io/Bayesian_inference/hierarchical-models.html)

We are going to 

## Normal distribution with log(y)

### Centered parameterization **mmNlogyC**

P357 McElreath (first version).

mmNlogyC (multi-level model with normal distribution and log(y) - centered paramerization)

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
    \mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{PROV[p]}\\
    \alpha_{PROV} & \sim \mathcal{N}(\mu_{\alpha_{PROV}},\sigma_{\alpha_{PROV}})\\
    \mu_{\alpha_{PROV}} & \sim \mathcal{N}(0,1)\\
    \sigma_{\alpha_{PROV}}& \sim \text{Exponential}(1)\\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r ReducedDatalist}
data.list.reduced <- list(N=length(data$height),         # Number of observations
                  y=log(data$height),                    # Log(Response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  prov=as.numeric(data$prov))            # Provenances
```

```{r SamplingmmNlogyC}
mmNlogyC <- stan_model("mmNlogyC.stan")  
fit.mmNlogyC <- sampling(mmNlogyC, data = data.list.reduced , iter = 2000, chains = 2, 
                                 cores = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mmNlogyC,
                pars = c("beta_age", "beta_age2", "mean_alpha_prov", "sigma_alpha_prov",
                         "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "One-varying intercept model mmNlogyC with centered parameterization")
```

This model has some divergent transitions, low number of effective sample size and R-hat not exactly equal to one.

> Divergent transitions


- To avoid divergent transitions, we can increase `adapt_delta`.. 

McEleath (Second version) : "[...] the target acceptance rate is controlled by the `adapt_delta` control parameter. The default is 0.95, which means that it aims to attain a 95% acceptance rate. It tries this during the warmup phase, adjusting the step size of each leapfrog step (go back to Chapter 9 if these terms aren’t familiar). When `adapt_delta` is set high, it results in a smaller step size, which means a more accurate approximation of the curved surface. It also means more computation, which means a slower chain. Increasing adapt_delta can often, but not always, help with divergent transitions."

- You can also try to add some information in your model, for instance with $\beta_{age} \sim \text{LogNormal}(0,1)$.

- Or you can reparameterize your model with the **non-centered parameterization**. (Best option!!)



```{r PostPredictmmNlogyC}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmNlogyC, pars = "y_rep")[1:50, ]) + 
  theme_bw() +
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r caterpillarmmNlogyC,fig.width=12}
mcmc_trace(as.array(fit.mmNlogyC), pars = c("alpha_prov[3]","sigma_alpha_prov"), 
           np = nuts_params(fit.mmNlogyC)) + 
  xlab("Post-warmup iteration")
```

```{r pairsPlotsmmNlogyC,fig.width=17, fig.height=10}
mcmc_pairs(as.array(fit.mmNlogyC), np = nuts_params(fit.mmNlogyC), 
           pars = c("beta_age","beta_age2","mean_alpha_prov","sigma_alpha_prov","alpha_prov[3]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=2, div_shape = 19))
```



### Non-centered parameterization **mmNlogyNC**

Links: 

- [Stan user guide](https://mc-stan.org/docs/2_22/stan-users-guide/reparameterization-section.html)

- https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html

- https://jrnold.github.io/bugs-examples-in-stan/aspirin.html


***

From McElreath, P429 (13.4.2.) of Statistical Rethinking (second version) 


$$ \alpha \sim \mathcal{N}(\mu,\sigma)$$


is equivalent to 

\begin{equation}
  \begin{aligned}
    \alpha &= \mu + \beta\\
    \beta &\sim \mathcal{N}(0,\sigma)
  \end{aligned}
\end{equation}

is equivalent to

\begin{equation}
  \begin{aligned}
    \alpha &= \mu + z\sigma\\
    z &\sim \mathcal{N}(0,1)
  \end{aligned}
\end{equation}

No parameters are left inside the prior.

***

From [Updating: A Set of Bayesian Notes. Jeffrey B. Arnold. 20 Multilevel Models](https://jrnold.github.io/bayesian_notes/multilevel-models.html)

These are two ways of writing the same model. However, they change the parameters that the HMC algorithm is actively sampling and thus can have different sampling performance.

However, neither is universally better.

  - If much data, the non-centered parameterization works better
  - If less data, the centered parameterization works better

And there is currently no ex-ante way to know which will work better, and at what amount of “data” that the performance of one or the other is better. However, one other reason to use the centered parameterization (if it is also scaled), is that the Stan HMC implementation tends to be more efficient if all parameters are on the scale.

***

mmNlogyNC (multi-level model with normal distribution and log(y) - non-centered paramerization)

> Mathematical model

\begin{equation}
  \begin{aligned}
    \text{log}(h_{i}) & \sim \mathcal{N}(log(\mu_{i}),\sigma_{i})\\
    \mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{PROV[p]}\sigma_{PROV}\\
    \alpha & \sim \mathcal{N}(0,1)\\
    z_{PROV} & \sim \mathcal{N}(0,1)\\
    \sigma_{\alpha_{PROV}}& \sim \text{Exponential}(1)\\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}


```{r SamplingmmNlogyNC}
mmNlogyNC <- stan_model("mmNlogyNC.stan")
fit.mmNlogyNC <- sampling(mmNlogyNC, data = data.list.reduced, iter = 2000, chains = 2,
                          cores = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mmNlogyNC,
                pars = c("beta_age", "beta_age2", "z_prov", "sigma_prov","alpha",
                         "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "One-varying intercept model mmNlogyNC with non-centered parameterization")
```

> No more divergent transitions! But still low effective sample size. 


```{r PostPredictmmNlogyNC}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmNlogyNC, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r caterpillarmmNlogyNC, fig.width = 16}
mcmc_trace(as.array(fit.mmNlogyNC), 
           pars =c( "z_prov[3]","sigma_prov"), 
           np = nuts_params(fit.mmNlogyNC)) + 
  xlab("Post-warmup iteration")
```

```{r pairsPlotsmmNlogyNC, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmNlogyNC), np = nuts_params(fit.mmNlogyNC),
           pars = c("beta_age","beta_age2","alpha","sigma_prov","z_prov[3]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


## Lognormal distribution

### Non-centered parameterization **mmLogNnc**

***

For a lognormal distribution the non-centered parameterisation is different:

$$\alpha \sim \mathcal{logN}(log(\mu),\sigma)$$

is equivalent to

\begin{equation}
  \begin{aligned}
    \alpha &=  e^{log(\mu) + z.\sigma} \\
    z &\sim \mathcal{N}(0,1)
  \end{aligned}
\end{equation}

***

mmLogNnc (multi-level LogNormal dsitribution - non-centered paramerization)

> Mathematical model

\begin{equation}
  \begin{aligned}
    h_{i} & \sim \text{LogNormal}(log(\mu_{i}),\sigma_{i})\\
    \mu_{i} & = e^{log(\alpha) + z_{PROV[p]}\sigma_{PROV}} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} \\
    \alpha & \sim \text{LogNormal}(0,1) \\
    \beta_{age} & \sim \mathcal{N}(0,1) \\
    \beta_{age2} & \sim \mathcal{N}(0,1)\\
    z_{PROV} & \sim \mathcal{N}(0,1)\\
    \sigma_{PROV} & \sim \text{Exponential}(1)\\
    \sigma & \sim \text{Exponential}(1)
  \end{aligned}
\end{equation}

```{r ReducedDatalistLogN}
data.list.reduced.logN <- list(N=length(data$height),         # Number of observations
                  y=data$height,                              # Response variable
                  age=data$age.sc,                            # Tree age
                  nprov=length(unique(data$prov)),            # Number of provenances
                  prov=as.numeric(data$prov))                 # Provenances
```

```{r SamplingmmLogNnc}
mmLogNnc<- stan_model("mmLogNnc.stan")
fit.mmLogNnc <- sampling(mmLogNnc, data = data.list.reduced.logN, iter = 2000, chains = 2,
                          cores = 2, control=list(max_treedepth=14), save_warmup = F)
broom::tidyMCMC(fit.mmLogNnc,
                pars = c("beta_age", "beta_age2", "z_prov", "sigma_prov","alpha",
                         "sigma_y", "lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "One-varying intercept model mmLogNnc with Lognormal non-centered parameterization")
```

```{r PostPredictmmLogNnc}
ppc_dens_overlay(y = data$height,
                 as.matrix(fit.mmLogNnc, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```


```{r caterpillarmmLogNnc, fig.width = 16}
mcmc_trace(as.array(fit.mmLogNnc), 
           pars =c( "z_prov[3]","sigma_prov"), 
           np = nuts_params(fit.mmLogNnc)) + 
  xlab("Post-warmup iteration")
```

```{r pairsPlotsmmLogNnc, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmLogNnc), np = nuts_params(fit.mmLogNnc),
           pars = c("beta_age","beta_age2","alpha","sigma_prov","z_prov[3]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


`r knitr::opts_chunk$set(eval = F)`

---
title: "Workshop 4: Hierarchical Models - Part 2"
author: "Juliette Archambeau, Sylvain Schmitt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    # code_fold: hide
    toc: true
    toc_depth: 5
    toc_float:
       collapsed: false
    number_sections: true
    theme: paper
    highlight: textmate
---


<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<style type="text/css">
body{ /* Normal  */
      font-size: 16px;
  }
div.main-container {
  max-width: 2000px;
  margin-left: auto;
  margin-right: auto;
}
</style>


```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 600px;
}
```

```{r setup, include=FALSE}
library(knitr)
options(width = 300)
knitr::opts_chunk$set(fig.width = 8, fig.height = 5, cache = TRUE)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(gridExtra)
library(grid)
library(ggplot2)
color_scheme_set("green")
library(tibble)
library(cowplot)
library(rethinking)
library(loo)
library(dplyr)
library(tidyr)
```



# **The data**

```{r loadData}
data <- readRDS(file="../data/sub_portugal_5provs_4blocks.rds") %>% 
  mutate_at(c("block", "prov", "clon", "tree"), as.factor) %>% # formatting data
  mutate(age.sc = as.vector(scale(age, center = F))) # as age is definite on R+ I would only reduce it..
  # mutate(age.sc = as.vector(scale(age))) # mean centering age
```

The dataset includes:

* Height data from a provenance trial (in Portugal) of maritime pine saplings. 
* Randomized block design. Here I selected 5 provenances and 4 blocks. 
* Saplings have different ages: 11, 15, 20 and 27 month old.

```{r expDesign}
table(data$prov,data$block) %>% kable(caption = "Provenance against block number.")
table(data$prov,as.factor(data$age)) %>% kable(caption = "Provenance against age (in months).")
```

```{r heightVsAge, message = F, warning = F, fig.cap="Height versus age."}
ggplot(data, aes(x=height)) + 
  geom_histogram(color="darkblue", fill="lightblue") + 
  theme_bw()

ggplot(data, aes(x=height, color=as.factor(age))) + 
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_wrap(~as.factor(age)) + 
  theme(legend.position = "none")

plot_grid(ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))

plot_grid(ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))
```

```{r heightProvBlock, message = F, warning = F, fig.cap="Height distribution by Provenance and Block.", fig.width=14,fig.height=8}
ggplot(data, aes(x=height, color=block)) +
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_grid(prov ~block, labeller = label_both) + 
  theme(legend.position = "none")
```



# **Two varying intercepts** (prov and block)

## Centered parameterization 

### Two global mean parameters **mm2NlogyCa**


\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{n}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha_{BLOCK} & \sim \mathcal{N}(\mu_{\alpha_{BLOCK}},\sigma_{\alpha_{BLOCK}})\\
\alpha_{PROV} & \sim \mathcal{N}(\mu_{\alpha_{PROV}},\sigma_{\alpha_{PROV}})\\
\mu_{\alpha_{PROV}} & \sim \mathcal{N}(0,1)\\
\mu_{\alpha_{BLOCK}}& \sim \mathcal{N}(0,1)\\
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)
\end{aligned}
\end{equation}


```{r Datalistmm2}
data.list.mm2 <- list(N=length(data$height),             # Number of observations
                  y=log(data$height),                    # log(response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  nblock=length(unique(data$block)),     # Number of blocks
                  prov=as.numeric(data$prov),            # Provenances
                  bloc=as.numeric(data$block))           # Blocks
```


```{r Samplingmm2NlogyCa}
mm2NlogyCa = stan_model("mm2NlogyCa.stan")
fit.mm2NlogyCa <- sampling(mm2NlogyCa, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyCa,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", 
                           "mean_alpha_prov","sigma_alpha_prov",
                           "mean_alpha_block","sigma_alpha_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercept model mm2NlogyCa with centered parameterization")
```

```{r PostPredictmm2NlogyCa}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyCa, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyCa, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCa), np = nuts_params(fit.mm2NlogyCa),
           pars = c("mean_alpha_prov","sigma_alpha_prov",
                    "mean_alpha_block","sigma_alpha_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

McElreath: "[...] We can’t identify a separate mean for each varying intercept type, because both intercepts are added to the same linear prediction. So it is conventional to define varying intercepts with a mean of zero, so there’s no risk of accidentally creating hard-to-identify parameters." "If you do include a mean for each cluster type, it won’t be the end of the world, however."



### One global mean parameter **mm2NlogyCb**

Same model but with one global mean parameter $\alpha$, and both of the varying intercept parameters ($\alpha_{BLOCK}$ and $\alpha_{PROV}$) are centered at zero.

\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha & \sim \mathcal{N}(0,1)\\
\alpha_{BLOCK} & \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})\\
\alpha_{PROV} & \sim \mathcal{N}(0,\sigma_{\alpha_{PROV}})\\
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)\\
\end{aligned}
\end{equation}

```{r Samplingmm2NlogyCb}
mm2NlogyCb = stan_model("mm2NlogyCb.stan")
fit.mm2NlogyCb <- sampling(mm2NlogyCb, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyCb,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", "alpha",
                           "sigma_alpha_prov","sigma_alpha_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercept model mm2NlogyCb with centered parameterization")
```




```{r PostPredictmm2NlogyCb}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyCb, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyCb, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha","sigma_alpha_prov","sigma_alpha_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

> The funnel geometry

The Hamiltonian Markov chain might explore the funnel neck less easily.

```{r pairsPlotsmm2NlogyCbFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]","sigma_alpha_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


[Betancourt](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)

## Non-centered parameterization **mm2NlogyNC**


\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{BLOCK[b]}\sigma_{BLOCK} + z_{PROV[p]}\sigma_{PROV}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha & \sim \mathcal{N}(0,1)\\
z_{BLOCK} & \sim \mathcal{N}(0,1)\\
z_{PROV} & \sim \mathcal{N}(0,1)\\
\sigma_{PROV} & \sim \text{Exponential}(1)\\
\sigma_{BLOCK} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)\\
\end{aligned}
\end{equation}



```{r Samplingmm2NlogyNC}
mm2NlogyNC = stan_model("mm2NlogyNC.stan")
fit.mm2NlogyNC <- sampling(mm2NlogyNC, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyNC,
                 pars = c("beta_age","beta_age2",
                           "z_prov","z_block", "alpha",
                           "sigma_prov","sigma_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercept model mm2NlogyNC with centered parameterization")
```


```{r PostPredictmm2NlogyNC}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyNC, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyNC, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyNC), np = nuts_params(fit.mm2NlogyNC),
           pars = c("alpha","sigma_prov","sigma_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


```{r pairsPlotsmm2NlogyNCFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyNC), np = nuts_params(fit.mm2NlogyNC),
           pars = c("z_prov[1]","z_prov[2]","z_prov[3]","z_prov[4]","z_prov[5]","sigma_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



> Increasing acceptance rate? 

```{r mm2NlogyNCAdaptDelta, fig.width = 17, fig.height = 10}
fit.mm2NlogyNC <- sampling(mm2NlogyNC, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14,adapt_delta=0.99))
mcmc_pairs(as.array(fit.mm2NlogyNC), np = nuts_params(fit.mm2NlogyNC),
           pars = c("z_prov[1]","z_prov[2]","z_prov[3]","z_prov[4]","z_prov[5]","sigma_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



# **Multilevel models with varying intercepts and slopes**

> P439 of McElreath (Second edition)

Provenances vary in their **average** height but may also have **different relationship** with age. The linear model might look like this:

$$\mu_{i} = \alpha_{PROV[p]} + \beta_{PROV[p]}age_{i} $$

Like before with the varying intercepts, we want to pool information about slopes, by estimating the variance of the slopes.

One important point however: provenances are likely to covary in their intercepts and slopes. For instance, we may hypothese that higher provenances on average are also those that grow faster. So we want a way to pool information across parameter types (intercepts and slopes).

"How should the robot[=the model] pool information across intercepts and slopes? By modeling the **joint population of intercepts and slopes**, which means by **modeling their covariance**. In conventional multilevel models, the device that makes this possible is a **joint multivariate Gaussian distribution** for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension)."

"**Why Gaussian?** There is no reason the multivariate distribution of intercepts and slopes must be Gaussian. But there are both practical and epistemological justifications. On the practical side, there aren’t many multivariate distributions that are easy to work with. The only common ones are multivariate Gaussian and multivariate Student (or “t”) distributions. On the epistemological side, if all we want to say about these intercepts and slopes is their means, variances, and covariances, then the maximum entropy distribution is multivariate Gaussian."


## Centered parameterization **mmCovNlogyClkj2**


\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + \alpha_{BLOCK[b]} + \alpha_{PROV[p]} + \beta_{PROV[p]}age_{i} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} \\[4pt]

\begin{bmatrix}  \alpha_{PROV} \\  \beta_{PROV}  
    \end{bmatrix} & \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{S} \right) \\[4pt]

\mathbf{S} & = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} 
          \mathbf{R}
          \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} \\[4pt]
          
\alpha & \sim \text{LogNormal}(0,1)\\[4pt] 
\beta_{age} & \sim \text{LogNormal}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]


\alpha_{BLOCK} & \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})\\[4pt]

\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\mathbf{R} & \sim \text{LKJcorr(2)}

\end{aligned}
\end{equation}

***

> Details from McEleath's book, P445, 14.1.3. The varying slopes model.


- $\begin{bmatrix}  \alpha_{PROV} \\  \beta_{PROV} \end{bmatrix} \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{S} \right)$ 

This line states that each provenance has an intercept $\alpha_{PROV}$ and slope $\beta_{PROV}$ with a prior distribution defined by the two-dimensional Gaussian distribution with mean 0 and covariance matrix $\mathbf{S}$. This statement of prior will adaptively regularize the individual intercepts, slopes, and the correlation among them.

- $\mathbf{S} = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} \mathbf{R} \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix}$. 

$\mathbf{S}$ is the covariance matrix and can be written by factoring it into separate standard deviations, $\sigma_{\alpha_{PROV}}$ and $\sigma_{\beta_{PROV}}$, and a correlation matrix $\mathbf{R}$. $\mathbf{R}$ can be written as follows:
$$\mathbf{R} = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$$
where $\rho$ is the correlation between intercepts and slopes.

- The rest of the model just defines fixed priors. The prior of $\mathbf{R}$ is the LKJcorr prior, with only one parameter to define it. In larger matrices, with additional varying slopes, the same LKJcorr prior will work. What LKJcorr(2) does is define a weakly informative prior on $\rho$ that is skeptical of extreme correlations near −1 or 1. You can think of it as a **regularizing prior for correlations**. This distribution has a single parameter, $\eta$, that controls how skeptical the prior is of large correlations in the matrix. When we use LKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, such as the 2 we used above, then extreme correlations are less likely.

```{r VizualizePriorLKJcorr,fig.width=4,fig.height=3}
Rho2 <- rlkjcorr( 1e4 , K=2 , eta=2 )
Rho1 <- rlkjcorr( 1e4 , K=2 , eta=1 )
Rho4 <- rlkjcorr( 1e4 , K=2 , eta=4 )
plot_grid(dens(Rho1[,1,2],xlim=c(-1,1),xlab="correlation" ,ylim=c(0,1.2),main="eta=1"),
         dens(Rho2[,1,2] , xlab="correlation" ,xlim=c(-1,1),ylim=c(0,1.2),main="eta=2"),
         dens(Rho4[,1,2],xlim=c(-1,1),xlab="correlation" ,ylim=c(0,1.2),main="eta=4"))
```


> So, how is it in Stan? 

The following model is based on: [Stan code of Statistical Rethinking. 13.3 Example: cross-classified chimpanzees with varying slopes](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd). But, take care, in this example, some priors are missing!


```{r DatalistmmCov}
data.list.mmCov <- list(N=length(data$height),           # Number of observations
                  y=log(data$height),                    # log(response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  nblock=length(unique(data$block)),     # Number of blocks
                  prov=as.numeric(data$prov),            # Provenances
                  bloc=as.numeric(data$block))           # Blocks
```


```{r SamplingmmCovNlogyClkj2}
mmCovNlogyClkj2 = stan_model("mmCovNlogyClkj2.stan")
fit.mmCovNlogyClkj2 <- sampling(mmCovNlogyClkj2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovNlogyClkj2,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyClkj2 with varying intercepts and slopes with centered parameterization")
```


```{r PostPredictmmCovNlogyClkj2}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovNlogyClkj2, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyClkj2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyClkj2), np = nuts_params(fit.mmCovNlogyClkj2),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmmCovNlogyClkj2AlphaBeta, fig.width = 17, fig.height = 15}
mcmc_pairs(as.array(fit.mmCovNlogyClkj2), np = nuts_params(fit.mmCovNlogyClkj2),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]",
                    "beta_prov[1]","beta_prov[2]","beta_prov[3]","beta_prov[4]","beta_prov[5]"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



Here the prior of the correlation matrix  $\mathbf{R}$ was $\text{LKJcorr(2)}$. We can try a more regularizing prior, which penalizes more the large correlations, for instance $\text{LKJcorr(4)}$. I saw that in `brms`, a package based on stan (more  usser-firendly for those who don't want to code their models in Stan) that they use $\text{LKJcorr(4)}$.


```{r SamplingmmCovNlogyClkj4}
mmCovNlogyClkj4 = stan_model("mmCovNlogyClkj4.stan")
fit.mmCovNlogyClkj4 <- sampling(mmCovNlogyClkj4, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovNlogyClkj4,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyClkj4 with varying intercepts and slopes with centered parameterization")
```


```{r PostPredictmmCovNlogyClkj4}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovNlogyClkj4, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyClkj4, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyClkj4), np = nuts_params(fit.mmCovNlogyClkj4),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


> We will use $\text{LKJcorr(4)}$ in all the subsequent models. 


## Non-centered parameterization 

### Version 1 **mmCovlogyNC1**

> P408 McElreath Statistical rethinking (First version)

> [stan code here](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd#L580)



\begin{equation}
\begin{aligned}
h_{i} & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + z_{\alpha_{BLOCK[b]}}\sigma_{\alpha_{BLOCK}} + z_{\alpha_{PROV[p]}}\sigma_{\alpha_{PROV}} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{\beta_{PROV[p]}}\sigma_{\beta_{PROV}}age_{i} \\[4pt]

\begin{bmatrix}  z_{\alpha_{PROV}} \\  z_{\beta_{PROV}} 
    \end{bmatrix} & \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{R} \right) \\[4pt]

          
          
\beta_{age} & \sim \mathcal{N}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]
\alpha & \sim \mathcal{N}(0,1)\\[4pt]

z_{\alpha_{BLOCK}} & \sim \mathcal{N}(0,1)\\[4pt]
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]

\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]


\mathbf{R}& \sim \text{LKJcorr(4)}

\end{aligned}
\end{equation}

Based on McEleath: "**Notice that each varying intercept and slope is multiplied by its corresponding scale parameter**, one of the standard deviations that ordinarily appears inside the multivariate Gaussian prior for these effects. This just undoes the standardization that is imposed in the prior. Inside the priors, there are no covariance matrices now. There are just correlation matrices, since the scale of each dimension has been migrated to the linear model. So these adaptive priors are essentially distributions of correlated z-scores."

```{r SamplingmmCovNlogyNC1}
mmCovlogyNC1 = stan_model("mmCovNlogyNC1.stan")
fit.mmCovlogyNC1 <- sampling(mmCovlogyNC1, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovlogyNC1,
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block","z_alpha_prov",
                          "z_beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovlogyNC1 with varying intercepts and slopes with non-centered parameterization")
```


```{r PostPredictmmCovNlogyNC1}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovlogyNC1, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyNC1, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovlogyNC1), np = nuts_params(fit.mmCovlogyNC1),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```






> Comment: The correlation is still in the matrix, **how can we take the correlation matrix out of the prior??**



### Cholesky decomposition version 


McElreath (p457, Second version): "It is possible to further extend this non-centering strategy of taking parameters out of the prior. All that is left in the priors above is the correlation matrix, $\mathbf{R}$ . These too can be extracted by using a **Cholesky decomposition** of the correlation matrix. A Cholesky decomposition $\mathbf{L}$ is a way to represent a square, symmetric matrix like a correlation matrix $\mathbf{R}$ such that $\mathbf{R} = \mathbf{L}\mathbf{L}^\intercal$ . It is a marvelous fact that you can multiply $\mathbf{L}$ by a matrix of uncorrelated samples (z-scores) and end up with a matrix of correlated samples (the varying effects). This is the trick that lets us take the covariance matrix out of the prior. We just sample a matrix of uncorrelated z-scores and then multiply those by the Cholesky factor and the standard deviations to get the varying effects with the correct scale and correlation. It would be magic, except that it is just algebra.""

> Let's look at three ways to write the same model. I think they are equivalent, it's just a question of writing.

#### Option 1 **mmCovNlogyNCcholdec1**

> Following McEleath P457 Second version of Statistical Rethinking.

```{r SamplingmmCovNlogyNCcholdec1}
mmCovNlogyNCcholdec1= stan_model("mmCovNlogyNCcholdec1.stan")
fit.mmCovNlogyNCcholdec1 <- sampling(mmCovNlogyNCcholdec1, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec1, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "v_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec1 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec1, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec1), np = nuts_params(fit.mmCovNlogyNCcholdec1),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


#### Option 2 **mmCovNlogyNCcholdec2**

> [stan code](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd#L680)

```{r SamplingmmCovNlogyNCcholdec2}
mmCovNlogyNCcholdec2= stan_model("mmCovNlogyNCcholdec2.stan")
fit.mmCovNlogyNCcholdec2 <- sampling(mmCovNlogyNCcholdec2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec2, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "alpha_prov","beta_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec2 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec2), np = nuts_params(fit.mmCovNlogyNCcholdec2),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


#### Option 3 **mmCovNlogyNCcholdec3**

> Similar to [Sorensen et al. 2016. Listing 8.](http://jakewestfall.org/misc/SorensenEtAl.pdf)

```{r SamplingmmCovNlogyNCcholdec3}
mmCovNlogyNCcholdec3= stan_model("mmCovNlogyNCcholdec3.stan")
fit.mmCovNlogyNCcholdec3 <- sampling(mmCovNlogyNCcholdec3, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec3, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "v_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec3 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec3, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec3), np = nuts_params(fit.mmCovNlogyNCcholdec3),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

> Effective sample size comparison


```{r nEffCompare}
tibble("mmCovNlogyClkj4"=summary(fit.mmCovNlogyClkj4)$summary[1:25,"n_eff"],
"mmCovlogyNC1"=summary(fit.mmCovlogyNC1)$summary[1:25,"n_eff"],
"mmCovNlogyNCcholdec1"=summary(fit.mmCovNlogyNCcholdec1)$summary[1:25,"n_eff"]) %>% 
  tidyr::pivot_longer(everything(),names_to = "model", values_to = "n_eff") %>% 
  ggplot(aes(x=model, y=n_eff)) + 
  geom_boxplot() + theme_bw()
```


> Running time

```{r comparingRunningTimeMM}
lapply(lapply(c(fit.mmCovNlogyClkj4, fit.mmCovlogyNC1,fit.mmCovNlogyNCcholdec1), 
       get_elapsed_time), data.frame) %>% 
  bind_rows(.id = "model") %>% 
  mutate(model = recode_factor(model, 
                               "1" = "Model mmCovNlogyClkj4 - centered",
                               "2" = "Model mmCovlogyNC1 - non-centered",
                               "3" = "Model mmCovNlogyNCcholdec1 - non-centered with the Cholesky decomposition")) %>% 
  mutate(total = warmup + sample) %>% 
  arrange(total) %>% 
  kable(caption = "Model speed comparison of multilevels models with covarying intercepts and slopes")
```

`r knitr::opts_chunk$set(eval = F)`


---
title: "Workshop 4: Hierarchical Models - Part 2"
author: "Juliette Archambeau, Sylvain Schmitt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    # code_fold: hide
    toc: true
    toc_depth: 5
    toc_float:
       collapsed: false
    number_sections: true
    theme: paper
    highlight: textmate
---


<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<style type="text/css">
body{ /* Normal  */
      font-size: 16px;
  }
div.main-container {
  max-width: 2000px;
  margin-left: auto;
  margin-right: auto;
}
</style>


```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 600px;
}
```

```{r setup, include=FALSE}
library(knitr)
options(width = 300)
knitr::opts_chunk$set(fig.width = 8, fig.height = 5, cache = TRUE)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(gridExtra)
library(grid)
library(ggplot2)
color_scheme_set("green")
library(tibble)
library(cowplot)
library(rethinking)
library(loo)
library(dplyr)
library(tidyr)
```



# **The data**

```{r loadData}
data <- readRDS(file="../data/sub_portugal_5provs_4blocks.rds") %>% 
  mutate_at(c("block", "prov", "clon", "tree"), as.factor) %>% # formatting data
  mutate(age.sc = as.vector(scale(age, center = F))) # as age is definite on R+ I would only reduce it..
  # mutate(age.sc = as.vector(scale(age))) # mean centering age
```

The dataset includes:

* Height data from a provenance trial (in Portugal) of maritime pine saplings. 
* Randomized block design. Here I selected 5 provenances and 4 blocks. 
* Saplings have different ages: 11, 15, 20 and 27 month old.

```{r expDesign}
table(data$prov,data$block) %>% kable(caption = "Provenance against block number.")
table(data$prov,as.factor(data$age)) %>% kable(caption = "Provenance against age (in months).")
```

```{r heightVsAge, message = F, warning = F, fig.cap="Height versus age."}
ggplot(data, aes(x=height)) + 
  geom_histogram(color="darkblue", fill="lightblue") + 
  theme_bw()

ggplot(data, aes(x=height, color=as.factor(age))) + 
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_wrap(~as.factor(age)) + 
  theme(legend.position = "none")

plot_grid(ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=height)) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))

plot_grid(ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red") + 
            theme_bw()  +
            theme(axis.title=element_text(size=16)),
          ggplot(data, aes(x=age,y=log(height))) + 
            geom_point(alpha=0.2) + 
            stat_smooth(method = "lm", col = "red", formula = y~poly(x,2)) + 
            theme_bw() +
            theme(axis.title=element_text(size=16)))
```

```{r heightProvBlock, message = F, warning = F, fig.cap="Height distribution by Provenance and Block.", fig.width=14,fig.height=8}
ggplot(data, aes(x=height, color=block)) +
  geom_histogram(fill="white", alpha=0.5, position="identity") + 
  theme_bw()  +
  facet_grid(prov ~block, labeller = label_both) + 
  theme(legend.position = "none")
```



# **Two varying intercepts** (prov and block)

## Centered parameterization 

### Two global mean parameters **mm2NlogyCa** (not to do)


\begin{equation}
\begin{aligned}
\text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha_{BLOCK} & \sim \mathcal{N}(\mu_{\alpha_{BLOCK}},\sigma_{\alpha_{BLOCK}})\\
\alpha_{PROV} & \sim \mathcal{N}(\mu_{\alpha_{PROV}},\sigma_{\alpha_{PROV}})\\
\mu_{\alpha_{PROV}} & \sim \mathcal{N}(0,1)\\
\mu_{\alpha_{BLOCK}}& \sim \mathcal{N}(0,1)\\
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)
\end{aligned}
\end{equation}


```{r Datalistmm2}
data.list.mm2 <- list(N=length(data$height),             # Number of observations
                  y=log(data$height),                    # log(response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  nblock=length(unique(data$block)),     # Number of blocks
                  prov=as.numeric(data$prov),            # Provenances
                  bloc=as.numeric(data$block))           # Blocks
```


```{r Samplingmm2NlogyCa}
mm2NlogyCa = stan_model("mm2NlogyCa.stan")
fit.mm2NlogyCa <- sampling(mm2NlogyCa, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyCa,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", 
                           "mean_alpha_prov","sigma_alpha_prov",
                           "mean_alpha_block","sigma_alpha_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercepts model mm2NlogyCa with centered parameterization")
```

```{r PostPredictmm2NlogyCa}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyCa, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyCa, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCa), np = nuts_params(fit.mm2NlogyCa),
           pars = c("mean_alpha_prov","sigma_alpha_prov",
                    "mean_alpha_block","sigma_alpha_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

McElreath: "[...] We can’t identify a separate mean for each varying intercept type, because both intercepts are added to the same linear prediction. So it is conventional to define varying intercepts with a mean of zero, so there’s no risk of accidentally creating **hard-to-identify parameters**." "If you do include a mean for each cluster type, it won’t be the end of the world, however."



### One global mean parameter **mm2NlogyCb**

Same model but with one global mean parameter $\alpha$, and both of the varying intercept parameters ($\alpha_{BLOCK}$ and $\alpha_{PROV}$) are centered at zero.

\begin{equation}
\begin{aligned}
\text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  \alpha_{BLOCK[b]} + \alpha_{PROV[p]}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha & \sim \mathcal{N}(0,1)\\
\alpha_{BLOCK} & \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})\\
\alpha_{PROV} & \sim \mathcal{N}(0,\sigma_{\alpha_{PROV}})\\
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)\\
\end{aligned}
\end{equation}

```{r Samplingmm2NlogyCb}
mm2NlogyCb = stan_model("mm2NlogyCb.stan")
fit.mm2NlogyCb <- sampling(mm2NlogyCb, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyCb,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", "alpha",
                           "sigma_alpha_prov","sigma_alpha_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercepts model mm2NlogyCb with centered parameterization")
```


```{r PostPredictmm2NlogyCb}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyCb, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyCb, fig.width = 17, fig.height = 8}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha","sigma_alpha_prov","sigma_alpha_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


> **The funnel geometry**

What happens when some parameters are conditional on other parameters? Like here for instance with the **adaptive priors**: $\alpha_{PROV} \sim \mathcal{N}(0,\sigma_{\alpha_{PROV}})$ and $\alpha_{BLOCK} \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})$.



```{r pairsPlotsmm2NlogyCbFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]","sigma_alpha_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmm2NlogyCbFunnelBlocks, fig.width = 17, fig.height = 8}
mcmc_pairs(as.array(fit.mm2NlogyCb), np = nuts_params(fit.mm2NlogyCb),
           pars = c("alpha_block[1]","alpha_block[2]","alpha_block[3]","alpha_block[4]","sigma_alpha_block"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

What we observe is that when $\sigma_{\alpha_{PROV}}$ or $\sigma_{\alpha_{BLOCK}}$ get small, $\alpha_{PROV}$ or $\alpha_{BLOCK}$ contract. This very narrow valley is called the funnel.
A big problem here is that there is no signle step size which can efficiently explore both of those regions of the posterior distribution. In the valley, we want very small step size.And in bigger areas, we want bigger step size to effectively explore it. This kind of posterior distribution are much more challenging, for all kind of samplers. Stan handles these posterior distributions quite well but when we increase the dimensionality like in hierarchical models, you have many parameters (here $\sigma_{\alpha_{PROV}}$ or $\sigma_{\alpha_{BLOCK}}$) that are conditional on one parameter (here $\alpha_{PROV}$ or $\alpha_{BLOCK}$). In this case, the Hamiltonian Markov chain might explore the funnel less easily. What can we do?

- Increase adapt_delta control parameter  => better step size adaptation + slower exploration. 

- re-parameterize!

[McElreath](https://www.youtube.com/watch?v=ZG3Oe35R5sY)

[Betancourt](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)


## Non-centered parameterization **mm2NlogyNCa**


\begin{equation}
\begin{aligned}
\text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\
\mu_{i} & = \alpha + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{BLOCK[b]}\sigma_{BLOCK} + z_{PROV[p]}\sigma_{PROV}\\
\beta_{age} & \sim \mathcal{N}(0,1) \\
\beta_{age2} & \sim \mathcal{N}(0,1)\\
\alpha & \sim \mathcal{N}(0,1)\\
z_{BLOCK} & \sim \mathcal{N}(0,1)\\
z_{PROV} & \sim \mathcal{N}(0,1)\\
\sigma_{PROV} & \sim \text{Exponential}(1)\\
\sigma_{BLOCK} & \sim \text{Exponential}(1)\\
\sigma & \sim \text{Exponential}(1)\\
\end{aligned}
\end{equation}



```{r Samplingmm2NlogyNCa}
mm2NlogyNCa = stan_model("mm2NlogyNCa.stan")
fit.mm2NlogyNCa <- sampling(mm2NlogyNCa, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyNCa,
                 pars = c("beta_age","beta_age2",
                           "z_prov","z_block", "alpha",
                           "sigma_prov","sigma_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercepts model mm2NlogyNCa with non-centered parameterization")
```


```{r PostPredictmm2NlogyNCa}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mm2NlogyNCa, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmm2NlogyNCa, fig.width = 17, fig.height = 8}
mcmc_pairs(as.array(fit.mm2NlogyNCa), np = nuts_params(fit.mm2NlogyNCa),
           pars = c("alpha","sigma_prov","sigma_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


```{r pairsPlotsmm2NlogyNCaFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyNCa), np = nuts_params(fit.mm2NlogyNCa),
           pars = c("z_prov[1]","z_prov[2]","z_prov[3]","z_prov[4]","z_prov[5]","sigma_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmm2NlogyNCaFunnelBlocks, fig.width = 17, fig.height = 8}
mcmc_pairs(as.array(fit.mm2NlogyNCa), np = nuts_params(fit.mm2NlogyNCa),
           pars = c("z_block[1]","z_block[2]","z_block[3]","z_block[4]","sigma_block"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


### Same but with getting the intercepts back **mm2NlogyNCb**

```{r Samplingmm2NlogyNCb}
mm2NlogyNCb = stan_model("mm2NlogyNCb.stan")
fit.mm2NlogyNCb <- sampling(mm2NlogyNCb, data = data.list.mm2, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mm2NlogyNCb,
                 pars = c("beta_age","beta_age2",
                           "alpha_prov","alpha_block", "alpha",
                           "sigma_prov","sigma_block",
                           "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Two-varying intercepts model mm2NlogyNCb with non-centered parameterization")
```

```{r pairsPlotsmm2NlogyNCb, fig.width = 17, fig.height = 8}
mcmc_pairs(as.array(fit.mm2NlogyNCb), np = nuts_params(fit.mm2NlogyNCb),
           pars = c("alpha","sigma_prov","sigma_block","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmm2NlogyNCbFunnel, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mm2NlogyNCb), np = nuts_params(fit.mm2NlogyNCb),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]","sigma_prov"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmm2NlogyNCbFunnelBlocks, fig.width = 17, fig.height = 8}
mcmc_pairs(as.array(fit.mm2NlogyNCb), np = nuts_params(fit.mm2NlogyNCb),
           pars = c("alpha_block[1]","alpha_block[2]","alpha_block[3]","alpha_block[4]","sigma_block"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

# **Multilevel models with varying intercepts and slopes**

> P439 of McElreath (Second edition)

Provenances vary in their **average** height but may also have **different relationship** with age. The linear model might look like this:

$$\mu_{i} = \alpha_{PROV[p]} + \beta_{PROV[p]}age_{i} $$

Like before with the varying intercepts, we want to pool information about slopes, by estimating the variance of the slopes.

One important point however: provenances are likely to covary in their intercepts and slopes. For instance, we may hypothese that higher provenances on average are also those that grow faster. So we want a way to pool information across parameter types (intercepts and slopes).

"How should the robot[=the model] pool information across intercepts and slopes? By modeling the **joint population of intercepts and slopes**, which means by **modeling their covariance**. In conventional multilevel models, the device that makes this possible is a **joint multivariate Gaussian distribution** for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension)."

"**Why Gaussian?** There is no reason the multivariate distribution of intercepts and slopes must be Gaussian. But there are both practical and epistemological justifications. On the practical side, there aren’t many multivariate distributions that are easy to work with. The only common ones are multivariate Gaussian and multivariate Student (or “t”) distributions. On the epistemological side, if all we want to say about these intercepts and slopes is their means, variances, and covariances, then the maximum entropy distribution is multivariate Gaussian."


## Centered parameterization **mmCovNlogyClkj2**


\begin{equation}
\begin{aligned}
\text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + \alpha_{BLOCK[b]} + \alpha_{PROV[p]} + \beta_{PROV[p]}age_{i} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} \\[4pt]

\begin{bmatrix}  \alpha_{PROV} \\  \beta_{PROV}  
    \end{bmatrix} & \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{S} \right) \\[4pt]

\mathbf{S} & = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} 
          \mathbf{R}
          \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} \\[4pt]
          
\alpha & \sim \mathcal{N}(0,1)\\[4pt] 
\beta_{age} & \sim \mathcal{N}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]


\alpha_{BLOCK} & \sim \mathcal{N}(0,\sigma_{\alpha_{BLOCK}})\\[4pt]

\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\mathbf{R} & \sim \text{LKJcorr(2)}

\end{aligned}
\end{equation}

***

> Details from McEleath's book, P445, 14.1.3. The varying slopes model.


- $\begin{bmatrix}  \alpha_{PROV} \\  \beta_{PROV} \end{bmatrix} \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{S} \right)$ 

This line states that each provenance has an intercept $\alpha_{PROV}$ and slope $\beta_{PROV}$ with a prior distribution defined by the two-dimensional Gaussian distribution with mean 0 and covariance matrix $\mathbf{S}$. This statement of prior will adaptively regularize the individual intercepts, slopes, and the correlation among them.

- $\mathbf{S} = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix} \mathbf{R} \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix}$. 

$\mathbf{S}$ is the covariance matrix and can be written by factoring it into separate standard deviations, $\sigma_{\alpha_{PROV}}$ and $\sigma_{\beta_{PROV}}$, and a correlation matrix $\mathbf{R}$, which can be written as follows:
$$\mathbf{R} = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$$
where $\rho$ is the correlation between intercepts and slopes.

- The rest of the model just defines fixed priors. The prior of $\mathbf{R}$ is the LKJcorr prior, with only one parameter to define it. In larger matrices, with additional varying slopes, the same LKJcorr prior will work. What LKJcorr(2) does is define a weakly informative prior on $\rho$ that is skeptical of extreme correlations near −1 or 1. You can think of it as a **regularizing prior for correlations**. This distribution has a single parameter, $\eta$, that controls how skeptical the prior is of large correlations in the matrix. When we use LKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, such as the 2 we used above, then extreme correlations are less likely.

```{r VizualizePriorLKJcorr,fig.width=6,fig.height=5,echo=F}
tibble("2"=rlkjcorr( 1e4 , K=2 , eta=2 )[,1,2],
       "1" = rlkjcorr( 1e4 , K=2 , eta=1 )[,1,2],
       "4" = rlkjcorr( 1e4 , K=2 , eta=4 )[,1,2]) %>% 
       tidyr::pivot_longer(everything(),names_to = "Rho", values_to = "eta") %>% 
  ggplot(aes(x=eta, color=Rho)) +
  geom_density(size=1) +
  theme_bw() + labs(color="Value of eta") + 
  theme(legend.position = c(0.85,0.8))
```

- [Stan's manual - Multivariate Priors for Hierarchical Models](https://mc-stan.org/docs/2_22/stan-users-guide/multivariate-hierarchical-priors-section.html)

"The basic behavior of the LKJ correlation distribution is similar to that of a beta distribution. For $\eta=1$, the result is a **uniform distribution**. Despite being the identity over correlation matrices, the marginal distribution over the entries in that matrix (i.e., the correlations) is not uniform between -1 and 1. Rather, **it concentrates around zero as the dimensionality increases due to the complex constraints**. For $\eta > 1$, the density increasingly concentrates mass around the unit matrix, i.e., **favoring less correlation**. For  $\eta < 1$, it increasingly concentrates mass in the other direction, i.e., **favoring more correlation**. The LKJ prior may thus be used to control the expected amount of correlation among the parameters. For a discussion of decomposing a covariance prior into a prior on correlation matrices and an independent prior on scales, see Barnard, McCulloch, and Meng (2000)."

- The following model is based on: [Stan code of Statistical Rethinking. 13.3 Example: cross-classified chimpanzees with varying slopes](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd). But, take care, in this example, some priors are missing!


```{r DatalistmmCov}
data.list.mmCov <- list(N=length(data$height),           # Number of observations
                  y=log(data$height),                    # log(response variable)
                  age=data$age.sc,                       # Tree age
                  nprov=length(unique(data$prov)),       # Number of provenances
                  nblock=length(unique(data$block)),     # Number of blocks
                  prov=as.numeric(data$prov),            # Provenances
                  bloc=as.numeric(data$block))           # Blocks
```


```{r SamplingmmCovNlogyClkj2}
mmCovNlogyClkj2 = stan_model("mmCovNlogyClkj2.stan")
fit.mmCovNlogyClkj2 <- sampling(mmCovNlogyClkj2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovNlogyClkj2,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyClkj2 with varying intercepts and slopes with centered parameterization")
```

```{r PostPredictmmCovNlogyClkj2}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovNlogyClkj2, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyClkj2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyClkj2), np = nuts_params(fit.mmCovNlogyClkj2),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r pairsPlotsmmCovNlogyClkj2AlphaBeta, fig.width = 17, fig.height = 15}
mcmc_pairs(as.array(fit.mmCovNlogyClkj2), np = nuts_params(fit.mmCovNlogyClkj2),
           pars = c("alpha_prov[1]","alpha_prov[2]","alpha_prov[3]","alpha_prov[4]","alpha_prov[5]",
                    "beta_prov[1]","beta_prov[2]","beta_prov[3]","beta_prov[4]","beta_prov[5]"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```



Here the prior of the correlation matrix  $\mathbf{R}$ was $\text{LKJcorr(2)}$. We can try a more regularizing prior, which penalizes more the large correlations, for instance $\text{LKJcorr(4)}$. I saw that in `brms`, a package based on stan (more  usser-firendly for those who don't want to code their models in Stan) that they use $\text{LKJcorr(4)}$.


```{r SamplingmmCovNlogyClkj4}
mmCovNlogyClkj4 = stan_model("mmCovNlogyClkj4.stan")
fit.mmCovNlogyClkj4 <- sampling(mmCovNlogyClkj4, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovNlogyClkj4,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyClkj4 with varying intercepts and slopes with centered parameterization")
```


```{r PostPredictmmCovNlogyClkj4}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovNlogyClkj4, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyClkj4, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyClkj4), np = nuts_params(fit.mmCovNlogyClkj4),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


> We will use $\text{LKJcorr(4)}$ in all the subsequent models. 


## Non-centered parameterization 

### Version 1 **mmCovlogyNC1**

> P408 McElreath Statistical rethinking (First version)

> [stan code here](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd#L580)



\begin{equation}
\begin{aligned}
\text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + z_{\alpha_{BLOCK[b]}}\sigma_{\alpha_{BLOCK}} + z_{\alpha_{PROV[p]}}\sigma_{\alpha_{PROV}} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i} +  z_{\beta_{PROV[p]}}\sigma_{\beta_{PROV}}age_{i} \\[4pt]

\begin{bmatrix}  z_{\alpha_{PROV}} \\  z_{\beta_{PROV}} 
    \end{bmatrix} & \sim \text{MVNormal}\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\mathbf{R} \right) \\[4pt]

          
          
\beta_{age} & \sim \mathcal{N}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]
\alpha & \sim \mathcal{N}(0,1)\\[4pt]

z_{\alpha_{BLOCK}} & \sim \mathcal{N}(0,1)\\[4pt]
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]

\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]


\mathbf{R}& \sim \text{LKJcorr(4)}

\end{aligned}
\end{equation}

Based on McEleath: "**Notice that each varying intercept and slope is multiplied by its corresponding scale parameter**, one of the standard deviations that ordinarily appears inside the multivariate Gaussian prior for these effects. This just undoes the standardization that is imposed in the prior. Inside the priors, there are no covariance matrices now. There are just correlation matrices, since the scale of each dimension has been migrated to the linear model. So these adaptive priors are essentially distributions of correlated z-scores."

```{r SamplingmmCovNlogyNC1}
mmCovlogyNC1 = stan_model("mmCovNlogyNC1.stan")
fit.mmCovlogyNC1 <- sampling(mmCovlogyNC1, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovlogyNC1,
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block","z_alpha_prov",
                          "z_beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovlogyNC1 with varying intercepts and slopes with non-centered parameterization")
```


```{r PostPredictmmCovNlogyNC1}
ppc_dens_overlay(y = log(data$height),
                 as.matrix(fit.mmCovlogyNC1, pars = "y_rep")[1:50, ]) +
  theme_bw() + 
  theme(legend.text=element_text(size=25), legend.title=element_text(size=18),
        axis.text = element_text(size=18), legend.position = c(0.8,0.6))
```

```{r pairsPlotsmmCovNlogyNC1, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovlogyNC1), np = nuts_params(fit.mmCovlogyNC1),
           pars = c("alpha","R_prov[1,2]","sigma_block","sigma_prov[1]","sigma_prov[2]","sigma_y","z_alpha_prov[1]","z_beta_prov[1]","z_alpha_block[1]"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


```{r caterpillarmmCovNlogyNC1, fig.width = 17, fig.height = 8}
mcmc_trace(as.array(fit.mmCovlogyNC1), pars = c("alpha","R_prov[1,2]","sigma_block","sigma_prov[1]","sigma_prov[2]","sigma_y","z_alpha_prov[1]","z_beta_prov[1]","z_alpha_block[1]"), 
           np = nuts_params(fit.mmCovlogyNC1)) + 
  xlab("Post-warmup iteration")
```

#### Same but with getting the intercepts back **mmCovNlogyNC2**



```{r SamplingmmCovNlogyNC2}
mmCovlogyNC2 = stan_model("mmCovNlogyNC2.stan")
fit.mmCovlogyNC2 <- sampling(mmCovlogyNC2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovlogyNC2,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovlogyNC2 with varying intercepts and slopes with non-centered parameterization")
```

```{r pairsPlotsmmCovNlogyNC2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovlogyNC2), np = nuts_params(fit.mmCovlogyNC2),
           pars = c("alpha","R_prov[1,2]","sigma_block","sigma_prov[1]","sigma_prov[2]","sigma_y","alpha_prov[1]","beta_prov[1]","alpha_block[1]"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


```{r caterpillarmmCovNlogyNC2, fig.width = 17, fig.height = 8}
mcmc_trace(as.array(fit.mmCovlogyNC2), pars = c("alpha","R_prov[1,2]","sigma_block","sigma_prov[1]","sigma_prov[2]","sigma_y","alpha_prov[1]","beta_prov[1]","alpha_block[1]"), 
           np = nuts_params(fit.mmCovlogyNC2)) + 
  xlab("Post-warmup iteration")
```


#### More regularizing priors on scale parameters **mmCovNlogyNC3**

\begin{equation}
\begin{aligned}
\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\end{aligned}
\end{equation}

```{r SamplingmmCovNlogyNC3}
mmCovlogyNC3 = stan_model("mmCovNlogyNC3.stan")
fit.mmCovlogyNC3 <- sampling(mmCovlogyNC3, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14))
broom::tidyMCMC(fit.mmCovlogyNC3,
                 pars = c("beta_age","beta_age2","alpha",
                          "alpha_block","sigma_block","alpha_prov",
                          "beta_prov","R_prov","sigma_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovlogyNC3 with varying intercepts and slopes with non-centered parameterization")
```

```{r pairsPlotsmmCovNlogyNC3, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovlogyNC3), np = nuts_params(fit.mmCovlogyNC3),
           pars = c("alpha","R_prov[1,2]","sigma_block","sigma_prov[1]","sigma_prov[2]","sigma_y","alpha_prov[1]","beta_prov[1]","alpha_block[1]"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

```{r caterpillarmmCovNlogyNC3, fig.width = 17, fig.height = 8}
mcmc_trace(as.array(fit.mmCovlogyNC3), pars = c("alpha","R_prov[1,2]","sigma_block","sigma_prov[1]","sigma_prov[2]","sigma_y","alpha_prov[1]","beta_prov[1]","alpha_block[1]"), 
           np = nuts_params(fit.mmCovlogyNC3)) + 
  xlab("Post-warmup iteration")
```


> Comment: The correlation is still in the matrix, **how can we take the correlation matrix out of the prior??**



### Cholesky decomposition version 

- [Stan's manual: Optimization through Cholesky Factorization](https://mc-stan.org/docs/2_22/stan-users-guide/multivariate-hierarchical-priors-section.html)
"An even better solution, both in terms of **efficiency** and **numerical stability**, is to parameterize the model directly in terms of Cholesky factors of correlation matrices using the multivariate version of the non-centered parameterization."


- McElreath (p457, Second version): "It is possible to further extend this non-centering strategy of taking parameters out of the prior. All that is left in the priors above is the correlation matrix, $\mathbf{R}$ . These too can be extracted by using a **Cholesky decomposition** of the correlation matrix. A Cholesky decomposition $\mathbf{L}$ is a way to represent a square, symmetric matrix like a correlation matrix $\mathbf{R}$ such that $\mathbf{R} = \mathbf{L}\mathbf{L}^\intercal$ . It is a marvelous fact that you can multiply $\mathbf{L}$ by a matrix of uncorrelated samples (z-scores) and end up with a matrix of correlated samples (the varying effects). This is the trick that lets us take the covariance matrix out of the prior. We just sample a matrix of uncorrelated z-scores and then multiply those by the Cholesky factor and the standard deviations to get the varying effects with the correct scale and correlation. It would be magic, except that it is just algebra."

- Jim Savage [A few simple reparameterizations](http://modernstatisticalworkflow.blogspot.com/2017/07/a-few-simple-reparameterizations.html)

\begin{equation}
\begin{aligned}
\text{log}(h_{i}) & \sim \mathcal{N}(\mu_{i},\sigma_{i})\\[4pt]
\mu_{i} & = \alpha + z_{\alpha_{BLOCK[b]}}\sigma_{\alpha_{BLOCK}} + v_{\alpha_{PROV[p]}} + v_{\beta_{PROV[p]}}age_{i} + \beta_{age}age_{i} + \beta_{age2}age^{2}_{i}  \\[4pt]

\begin{bmatrix}  v_{\alpha_{PROV}} \\  v_{\beta_{PROV}}
    \end{bmatrix} & = \begin{pmatrix} \sigma_{\alpha_{PROV}} & 0 \\ 0 & \sigma_{\beta_{PROV}} \end{pmatrix}  \mathbf{L} \mathbf{L}^\intercal \begin{bmatrix}  z_{\alpha_{PROV}} \\  z_{\beta_{PROV}} \end{bmatrix} \\[4pt]
    
\begin{bmatrix}  z_{\alpha_{PROV}} \\  z_{\beta_{PROV}} 
    \end{bmatrix} & \sim \mathcal{N}(0,1) \\[4pt]

          
          
\beta_{age} & \sim \mathcal{N}(0,1) \\[4pt]
\beta_{age2} & \sim \mathcal{N}(0,1)\\[4pt]
\alpha & \sim \mathcal{N}(0,1)\\[4pt]

z_{\alpha_{BLOCK}} & \sim \mathcal{N}(0,1)\\[4pt]
\sigma_{\alpha_{BLOCK}} & \sim \text{Exponential}(1)\\[4pt]
\sigma & \sim \text{Exponential}(1)\\[4pt]

\sigma_{\beta_{PROV}} & \sim \text{Exponential}(1)\\[4pt]
\sigma_{\alpha_{PROV}} & \sim \text{Exponential}(1)\\[4pt]


\mathbf{R} = \mathbf{L} \mathbf{L}^\intercal &  \sim \text{LKJcorr(4)}

\end{aligned}
\end{equation}


> Let's look at three ways to write the same model. I think they are equivalent, it's just a matter of writing.

#### Option 1 **mmCovNlogyNCcholdec1**

> Following McEleath P457 Second version of Statistical Rethinking.

```{r SamplingmmCovNlogyNCcholdec1}
mmCovNlogyNCcholdec1= stan_model("mmCovNlogyNCcholdec1.stan")
fit.mmCovNlogyNCcholdec1 <- sampling(mmCovNlogyNCcholdec1, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec1, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "v_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec1 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version)")
```

```{r pairsPlotsmmCovNlogyNCcholdec1, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec1), np = nuts_params(fit.mmCovNlogyNCcholdec1),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


#### Option 2 **mmCovNlogyNCcholdec2**

> [stan code](https://github.com/ssp3nc3r/rethinking/blob/master/chapter13.Rmd#L680)

```{r SamplingmmCovNlogyNCcholdec2}
mmCovNlogyNCcholdec2= stan_model("mmCovNlogyNCcholdec2.stan")
fit.mmCovNlogyNCcholdec2 <- sampling(mmCovNlogyNCcholdec2, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec2, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "alpha_prov","beta_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec2 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec2, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec2), np = nuts_params(fit.mmCovNlogyNCcholdec2),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```


#### Option 3 **mmCovNlogyNCcholdec3**

> Similar to [Sorensen et al. 2016. Listing 8.](http://jakewestfall.org/misc/SorensenEtAl.pdf)

```{r SamplingmmCovNlogyNCcholdec3}
mmCovNlogyNCcholdec3= stan_model("mmCovNlogyNCcholdec3.stan")
fit.mmCovNlogyNCcholdec3 <- sampling(mmCovNlogyNCcholdec3, data = data.list.mmCov, iter = 2000, chains = 2, cores = 2, 
                           control=list(max_treedepth=14)) 
broom::tidyMCMC(fit.mmCovNlogyNCcholdec3, 
                 pars = c("beta_age","beta_age2","alpha",
                          "z_alpha_block","sigma_block",
                          "v_prov","sigma_prov","R_prov",
                          "sigma_y","lp__"),
                droppars = NULL, estimate.method = "median", ess = T, rhat = T, conf.int = T) %>%
  kable(caption = "Multilevel model mmCovNlogyNCcholdec3 with varying intercepts and slopes with non-centered parameterization (Cholesky decomposition version")
```

```{r pairsPlotsmmCovNlogyNCcholdec3, fig.width = 17, fig.height = 10}
mcmc_pairs(as.array(fit.mmCovNlogyNCcholdec3), np = nuts_params(fit.mmCovNlogyNCcholdec3),
           pars = c("alpha","R_prov[1,2]","sigma_prov[1]","sigma_prov[2]","sigma_y"), 
           off_diag_args = list(size = 1, alpha = 1/3),
           np_style = pairs_style_np(div_size=3, div_shape = 19))
```

## Model quick comparison

> Effective sample size 


```{r nEffCompare, warning=F,message=F,echo=F,fig.width=20,fig.height=6}
tibble(
"mmCovNlogyClkj2"=summary(fit.mmCovNlogyClkj2)$summary[1:25,"n_eff"],
"mmCovNlogyClkj4"=summary(fit.mmCovNlogyClkj4)$summary[1:25,"n_eff"],
"mmCovlogyNC1"=summary(fit.mmCovlogyNC1)$summary[1:25,"n_eff"],
"mmCovlogyNC2"=summary(fit.mmCovlogyNC2)$summary[1:25,"n_eff"],
"mmCovlogyNC3"=summary(fit.mmCovlogyNC3)$summary[1:25,"n_eff"],
"mmCovNlogyNCcholdec1"=summary(fit.mmCovNlogyNCcholdec1)$summary[1:25,"n_eff"],
"mmCovNlogyNCcholdec2"=summary(fit.mmCovNlogyNCcholdec2)$summary[1:25,"n_eff"],
"mmCovNlogyNCcholdec3"=summary(fit.mmCovNlogyNCcholdec3)$summary[1:25,"n_eff"]) %>% 
  tidyr::pivot_longer(everything(),names_to = "model", values_to = "n_eff") %>% 
  ggplot(aes(x=model, y=n_eff)) + 
  geom_boxplot() + theme_bw() + labs(y="Effective sample size") +
  theme(axis.text = element_text(size=14),axis.title = element_text(size=18))
```


> Running time

```{r comparingRunningTimeMM, echo=F}
lapply(lapply(c(fit.mmCovNlogyClkj2,fit.mmCovNlogyClkj4, 
                fit.mmCovlogyNC1,fit.mmCovlogyNC2,fit.mmCovlogyNC3,
                fit.mmCovNlogyNCcholdec1,fit.mmCovNlogyNCcholdec2,fit.mmCovNlogyNCcholdec3), 
       get_elapsed_time), data.frame) %>% 
  bind_rows(.id = "model") %>% 
  mutate(model = recode_factor(model, 
                               "1" = "Model mmCovNlogyClkj2 - centered $\\text{LKJcorr(2)}$",
                               "2" = "Model mmCovNlogyClkj4 - centered $\\text{LKJcorr(4)}$",
                               "3" = "Model mmCovlogyNC1 - non-centered",
                               "4" = "Model mmCovlogyNC2 - non-centered $\\sigma_{PROV} \\sim \\text{Exponential}(1)$",
                               "5" = "Model mmCovlogyNC3 - non-centered $\\sigma_{PROV} \\sim \\mathcal{N}(0,1)$",
                               "6" = "Model mmCovNlogyNCcholdec1 - non-centered with the Cholesky decomposition 1",
                               "7" = "Model mmCovNlogyNCcholdec2 - non-centered with the Cholesky decomposition 2",
                               "8" = "Model mmCovNlogyNCcholdec3 - non-centered with the Cholesky decomposition 3")) %>% 
  mutate(total = warmup + sample) %>% 
  arrange(total) %>% 
  kable(caption = "Model speed comparison of multilevels models with covarying intercepts and slopes")
```

`r knitr::opts_chunk$set(eval = F)`

